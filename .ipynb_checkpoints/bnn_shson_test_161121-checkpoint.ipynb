{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys, time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from bnn_shson import *\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_to_onehot(nums, n_labels):\n",
    "    results = list()\n",
    "    for i in range(len(nums)):\n",
    "        res = np.zeros([n_labels])\n",
    "        res[nums[i]] = 1\n",
    "        results.append(res)\n",
    "    return np.asarray(results, dtype = 'float32')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = h5py.File('mnist.hdf5', 'r')\n",
    "\n",
    "x_train = mnist['train_data'][()]\n",
    "t_train = num_to_onehot(mnist['train_label'][()], 10)\n",
    "x_valid = mnist['valid_data'][()]\n",
    "t_valid = num_to_onehot(mnist['valid_label'][()], 10)\n",
    "x_test = mnist['test_data'][()]\n",
    "t_test = num_to_onehot(mnist['test_label'][()], 10)\n",
    "\n",
    "mnist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blundell version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer done\n",
      "layer done\n",
      "(10, ?, 10)\n"
     ]
    }
   ],
   "source": [
    "#bnn = bnn_model(shape, mu = 0.1, rho = 0.1, n_samples = 10, outact = tf.sigmoid, seed = 1234, lr = 1e-8)\n",
    "bnn = bnn_model([784, 50, 10], size_data = len(t_train), size_batch = batch_size, \\\n",
    "                mu = 0.02, rho = -1, n_samples = 10, outact = tf.sigmoid, seed = 1234, \\\n",
    "                lr = 1e-3, kl_reweight = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0, step 0, training accuracy 0.12\n",
      "f : 158433.5, q : -395675.8125, p : -681588.875, l : 14897.1064453\n",
      "ep 0, step 50, training accuracy 0.14\n",
      "f : 5204.37011719, q : -393143.875, p : -684969.0, l : 5361.90478516\n",
      "ep 0, step 100, training accuracy 0.22\n",
      "f : 4744.19335938, q : -392972.125, p : -687887.0, l : 4748.96484375\n",
      "ep 0, step 150, training accuracy 0.365\n",
      "f : 4236.07666016, q : -393928.59375, p : -691311.375, l : 4316.70117188\n",
      "ep 0, step 200, training accuracy 0.485\n",
      "f : 3603.91210938, q : -392260.75, p : -691830.3125, l : 3807.90917969\n",
      "valid accuracy 0.4544\n",
      "ep 1, step 0, training accuracy 0.47\n",
      "f : 155175.84375, q : -392200.5, p : -695550.1875, l : 3339.85424805\n",
      "ep 1, step 50, training accuracy 0.555\n",
      "f : 3110.14355469, q : -390537.625, p : -693399.625, l : 3121.4765625\n",
      "ep 1, step 100, training accuracy 0.585\n",
      "f : 3002.48388672, q : -390206.8125, p : -695945.5625, l : 3048.96533203\n",
      "ep 1, step 150, training accuracy 0.58\n",
      "f : 3105.33666992, q : -390380.6875, p : -696804.375, l : 2978.86132812\n",
      "ep 1, step 200, training accuracy 0.735\n",
      "f : 2229.04199219, q : -389305.6875, p : -697991.5, l : 2185.16894531\n",
      "valid accuracy 0.7544\n",
      "ep 2, step 0, training accuracy 0.74\n",
      "f : 157162.96875, q : -389888.125, p : -702368.75, l : 2162.8269043\n",
      "ep 2, step 50, training accuracy 0.775\n",
      "f : 2134.3684082, q : -388876.125, p : -697036.3125, l : 2112.34033203\n",
      "ep 2, step 100, training accuracy 0.72\n",
      "f : 2146.37304688, q : -387945.71875, p : -699923.0, l : 2204.78491211\n",
      "ep 2, step 150, training accuracy 0.73\n",
      "f : 2351.05053711, q : -388028.34375, p : -700253.75, l : 2250.20019531\n",
      "ep 2, step 200, training accuracy 0.795\n",
      "f : 1658.99450684, q : -387666.5625, p : -701294.375, l : 1599.83666992\n",
      "valid accuracy 0.8154\n",
      "ep 3, step 0, training accuracy 0.815\n",
      "f : 159220.796875, q : -387654.375, p : -703627.125, l : 1854.55810547\n",
      "ep 3, step 50, training accuracy 0.825\n",
      "f : 1804.50170898, q : -386835.5625, p : -698129.375, l : 1774.19335938\n",
      "ep 3, step 100, training accuracy 0.775\n",
      "f : 1804.53063965, q : -386212.3125, p : -698115.125, l : 1888.6418457\n",
      "ep 3, step 150, training accuracy 0.77\n",
      "f : 1942.60510254, q : -386071.375, p : -699621.375, l : 1967.39416504\n",
      "ep 3, step 200, training accuracy 0.835\n",
      "f : 1469.99279785, q : -385914.65625, p : -702420.25, l : 1449.80773926\n",
      "valid accuracy 0.8421\n",
      "ep 4, step 0, training accuracy 0.81\n",
      "f : 159189.703125, q : -386288.375, p : -703019.5, l : 1617.88256836\n",
      "ep 4, step 50, training accuracy 0.85\n",
      "f : 1422.4732666, q : -385038.625, p : -698951.25, l : 1468.46826172\n",
      "ep 4, step 100, training accuracy 0.815\n",
      "f : 1724.69311523, q : -383694.96875, p : -698990.875, l : 1620.28369141\n",
      "ep 4, step 150, training accuracy 0.795\n",
      "f : 1756.51916504, q : -384982.1875, p : -697350.625, l : 1862.09179688\n",
      "ep 4, step 200, training accuracy 0.865\n",
      "f : 1261.02062988, q : -384514.78125, p : -699492.625, l : 1196.00830078\n",
      "valid accuracy 0.8604\n",
      "ep 5, step 0, training accuracy 0.855\n",
      "f : 159178.234375, q : -384362.875, p : -702013.5, l : 1472.39355469\n",
      "ep 5, step 50, training accuracy 0.845\n",
      "f : 1320.61108398, q : -383390.65625, p : -697197.625, l : 1290.4128418\n",
      "ep 5, step 100, training accuracy 0.8\n",
      "f : 1488.48571777, q : -384458.375, p : -697762.5, l : 1508.46362305\n",
      "ep 5, step 150, training accuracy 0.835\n",
      "f : 1629.77709961, q : -382978.25, p : -698256.75, l : 1593.83654785\n",
      "ep 5, step 200, training accuracy 0.875\n",
      "f : 1118.36035156, q : -382391.1875, p : -699741.0, l : 1138.82714844\n",
      "valid accuracy 0.8809\n",
      "ep 6, step 0, training accuracy 0.865\n",
      "f : 159589.0625, q : -382596.15625, p : -698751.625, l : 1347.09265137\n",
      "ep 6, step 50, training accuracy 0.845\n",
      "f : 1199.7902832, q : -382103.3125, p : -694680.875, l : 1142.48144531\n",
      "ep 6, step 100, training accuracy 0.845\n",
      "f : 1379.37060547, q : -381404.03125, p : -695860.6875, l : 1346.98693848\n",
      "ep 6, step 150, training accuracy 0.825\n",
      "f : 1439.11499023, q : -381514.8125, p : -697100.4375, l : 1434.04882812\n",
      "ep 6, step 200, training accuracy 0.885\n",
      "f : 960.376708984, q : -380756.84375, p : -698806.9375, l : 1056.70825195\n",
      "valid accuracy 0.8905\n",
      "ep 7, step 0, training accuracy 0.885\n",
      "f : 160125.03125, q : -381358.5625, p : -700957.875, l : 1196.06274414\n",
      "ep 7, step 50, training accuracy 0.86\n",
      "f : 1112.04248047, q : -380269.3125, p : -695050.6875, l : 1100.99755859\n",
      "ep 7, step 100, training accuracy 0.865\n",
      "f : 1339.54711914, q : -380289.3125, p : -694093.625, l : 1363.56262207\n",
      "ep 7, step 150, training accuracy 0.875\n",
      "f : 1360.36462402, q : -380497.875, p : -696276.8125, l : 1398.37072754\n",
      "ep 7, step 200, training accuracy 0.875\n",
      "f : 916.862792969, q : -379636.875, p : -696647.875, l : 918.394042969\n",
      "valid accuracy 0.8995\n",
      "ep 8, step 0, training accuracy 0.865\n",
      "f : 160036.859375, q : -380279.9375, p : -696325.5, l : 1163.44604492\n",
      "ep 8, step 50, training accuracy 0.88\n",
      "f : 1045.53662109, q : -378655.71875, p : -691151.125, l : 1076.11157227\n",
      "ep 8, step 100, training accuracy 0.86\n",
      "f : 1204.72192383, q : -378215.6875, p : -692140.1875, l : 1218.09570312\n",
      "ep 8, step 150, training accuracy 0.885\n",
      "f : 1301.41809082, q : -378496.96875, p : -693885.0, l : 1286.57714844\n",
      "ep 8, step 200, training accuracy 0.895\n",
      "f : 868.66809082, q : -378500.875, p : -695748.75, l : 896.965698242\n",
      "valid accuracy 0.9011\n",
      "ep 9, step 0, training accuracy 0.875\n",
      "f : 158998.515625, q : -377939.5, p : -694533.25, l : 1034.2800293\n",
      "ep 9, step 50, training accuracy 0.87\n",
      "f : 992.190551758, q : -376181.25, p : -690501.25, l : 1086.39501953\n",
      "ep 9, step 100, training accuracy 0.88\n",
      "f : 1142.4909668, q : -376800.9375, p : -692761.125, l : 1242.56982422\n",
      "ep 9, step 150, training accuracy 0.885\n",
      "f : 1180.34143066, q : -376135.5, p : -690745.1875, l : 1185.28894043\n",
      "ep 9, step 200, training accuracy 0.905\n",
      "f : 845.851501465, q : -375999.78125, p : -691389.4375, l : 888.087280273\n",
      "valid accuracy 0.9115\n",
      "ep 10, step 0, training accuracy 0.895\n",
      "f : 158758.265625, q : -376154.59375, p : -692340.375, l : 1014.81164551\n",
      "ep 10, step 50, training accuracy 0.895\n",
      "f : 974.376159668, q : -375976.59375, p : -687222.375, l : 1013.60638428\n",
      "ep 10, step 100, training accuracy 0.88\n",
      "f : 1123.6105957, q : -375596.8125, p : -687682.0, l : 1140.66699219\n",
      "ep 10, step 150, training accuracy 0.875\n",
      "f : 1141.77844238, q : -375128.5625, p : -689648.875, l : 1158.71411133\n",
      "ep 10, step 200, training accuracy 0.915\n",
      "f : 795.792175293, q : -374999.71875, p : -688892.875, l : 795.262084961\n",
      "valid accuracy 0.9084\n",
      "ep 11, step 0, training accuracy 0.895\n",
      "f : 158948.375, q : -374696.5, p : -689076.875, l : 961.38104248\n",
      "ep 11, step 50, training accuracy 0.88\n",
      "f : 1058.94567871, q : -374030.75, p : -685200.125, l : 1001.8961792\n",
      "ep 11, step 100, training accuracy 0.89\n",
      "f : 1121.64355469, q : -374820.0, p : -685776.375, l : 1144.67749023\n",
      "ep 11, step 150, training accuracy 0.875\n",
      "f : 1152.36682129, q : -374568.0, p : -688325.625, l : 1131.97607422\n",
      "ep 11, step 200, training accuracy 0.9\n",
      "f : 757.528015137, q : -373690.3125, p : -687815.625, l : 797.991088867\n",
      "valid accuracy 0.9132\n",
      "ep 12, step 0, training accuracy 0.9\n",
      "f : 157857.3125, q : -372892.625, p : -687781.875, l : 1000.30633545\n",
      "ep 12, step 50, training accuracy 0.9\n",
      "f : 897.275634766, q : -372744.5625, p : -683723.375, l : 979.764160156\n",
      "ep 12, step 100, training accuracy 0.87\n",
      "f : 1118.19030762, q : -372332.4375, p : -684144.0625, l : 1093.43212891\n",
      "ep 12, step 150, training accuracy 0.88\n",
      "f : 1189.20446777, q : -371943.1875, p : -685160.75, l : 1045.78100586\n",
      "ep 12, step 200, training accuracy 0.93\n",
      "f : 729.809326172, q : -371705.3125, p : -684918.75, l : 753.172912598\n",
      "valid accuracy 0.9165\n",
      "ep 13, step 0, training accuracy 0.905\n",
      "f : 158316.15625, q : -372300.1875, p : -685818.6875, l : 944.322143555\n",
      "ep 13, step 50, training accuracy 0.9\n",
      "f : 898.898071289, q : -370980.3125, p : -682027.1875, l : 887.847290039\n",
      "ep 13, step 100, training accuracy 0.895\n",
      "f : 1078.9498291, q : -371174.65625, p : -682060.5, l : 1112.89831543\n",
      "ep 13, step 150, training accuracy 0.88\n",
      "f : 1084.58996582, q : -371282.21875, p : -682546.375, l : 1109.87316895\n",
      "ep 13, step 200, training accuracy 0.925\n",
      "f : 725.459106445, q : -370600.25, p : -682672.4375, l : 743.46887207\n",
      "valid accuracy 0.9176\n",
      "ep 14, step 0, training accuracy 0.905\n",
      "f : 157504.5, q : -370647.375, p : -683018.4375, l : 912.251586914\n",
      "ep 14, step 50, training accuracy 0.905\n",
      "f : 925.661437988, q : -370276.09375, p : -679810.25, l : 842.885009766\n",
      "ep 14, step 100, training accuracy 0.9\n",
      "f : 1022.76623535, q : -369721.125, p : -679331.8125, l : 1019.65350342\n",
      "ep 14, step 150, training accuracy 0.895\n",
      "f : 1055.77160645, q : -370029.0, p : -679357.75, l : 1058.79992676\n",
      "ep 14, step 200, training accuracy 0.93\n",
      "f : 736.815307617, q : -370017.125, p : -678732.375, l : 712.008239746\n",
      "valid accuracy 0.9203\n",
      "ep 15, step 0, training accuracy 0.91\n",
      "f : 155698.546875, q : -369824.03125, p : -682114.0, l : 868.079040527\n",
      "ep 15, step 50, training accuracy 0.9\n",
      "f : 840.780883789, q : -368953.375, p : -676353.25, l : 945.630371094\n",
      "ep 15, step 100, training accuracy 0.905\n",
      "f : 1034.66772461, q : -367992.4375, p : -677256.5625, l : 1039.51855469\n",
      "ep 15, step 150, training accuracy 0.91\n",
      "f : 1078.85351562, q : -367981.5, p : -677343.75, l : 1020.95135498\n",
      "ep 15, step 200, training accuracy 0.925\n",
      "f : 668.014831543, q : -367592.625, p : -678091.5625, l : 655.442749023\n",
      "valid accuracy 0.9189\n",
      "ep 16, step 0, training accuracy 0.93\n",
      "f : 155933.46875, q : -367773.0, p : -680019.1875, l : 871.028442383\n",
      "ep 16, step 50, training accuracy 0.9\n",
      "f : 908.219360352, q : -367817.1875, p : -674163.4375, l : 890.799682617\n",
      "ep 16, step 100, training accuracy 0.89\n",
      "f : 988.225036621, q : -366865.875, p : -675487.625, l : 1044.38745117\n",
      "ep 16, step 150, training accuracy 0.905\n",
      "f : 1019.51428223, q : -366901.625, p : -674784.9375, l : 958.469360352\n",
      "ep 16, step 200, training accuracy 0.94\n",
      "f : 656.183654785, q : -366682.40625, p : -675695.5625, l : 689.10736084\n",
      "valid accuracy 0.9207\n",
      "ep 17, step 0, training accuracy 0.905\n",
      "f : 154898.328125, q : -366555.75, p : -676738.8125, l : 862.661987305\n",
      "ep 17, step 50, training accuracy 0.905\n",
      "f : 880.569946289, q : -366737.4375, p : -672289.375, l : 837.331481934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fe93fc865272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mbnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecay_klrw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#         if (i+1) % 20 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#             print(\"klrw index : %g\"%(bnn.get_klrw()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/bayes_nn_shson/bnn_shson.pyc\u001b[0m in \u001b[0;36mdecay_klrw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecay_klrw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoeff_kl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoeff_kl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_klrw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \"\"\"\n\u001b[1;32m--> 559\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3654\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3655\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3656\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    941\u001b[0m                 run_metadata):\n\u001b[0;32m    942\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[1;32m--> 992\u001b[1;33m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[0;32m    993\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "n_epochs = 100\n",
    "n_batches = len(t_train) / batch_size\n",
    "\n",
    "fs = list()\n",
    "qs = list()\n",
    "ps = list()\n",
    "ls = list()\n",
    "taccs = list()\n",
    "vaccs = list()\n",
    "for ep in range(n_epochs):\n",
    "    bnn.reset_klrw()\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        bnn.decay_klrw()\n",
    "#         if (i+1) % 20 == 0:\n",
    "#             print(\"klrw index : %g\"%(bnn.get_klrw()))\n",
    "        \n",
    "        feed = {bnn.x: x_train[i*batch_size:(i+1)*batch_size], \\\n",
    "                bnn.t: t_train[i*batch_size:(i+1)*batch_size]}\n",
    "\n",
    "        \n",
    "        v_f, v_q, v_p, v_l = bnn.get_fqpl(feed)\n",
    "        fs.append(v_f), qs.append(v_q), ps.append(v_p), ls.append(v_l)\n",
    "        \n",
    "#         if i > 50 and np.mean(fs[-50:-25]) < np.mean(fs[-25:]):\n",
    "#             bnn.decay_lr()\n",
    "#             print (\"--- learning rate decayed : %g ---\"%(bnn.get_lr()))\n",
    "\n",
    "        if i%50 == 0:\n",
    "            train_accuracy = bnn.validate(feed)\n",
    "\n",
    "            print(\"ep %d, step %d, training accuracy %g\"%(ep, i, train_accuracy))\n",
    "            print(\"f : {}, q : {}, p : {}, l : {}\".format(v_f, v_q, v_p, v_l))\n",
    "            #print v_q - v_p + v_l\n",
    "\n",
    "        bnn.train(feed)\n",
    "    \n",
    "    vacc = bnn.validate({bnn.x: x_valid, bnn.t: t_valid})\n",
    "    vaccs.append(vacc)\n",
    "    taccs.append(train_accuracy)\n",
    "    print(\"valid accuracy %g\"%vacc)\n",
    "    \n",
    "    if ep > 10 and np.mean(vaccs[-10:-5]) < np.mean(vaccs[-5:]):\n",
    "        bnn.decay_lr()\n",
    "        print (\"--- learning rate decayed : %g ---\"%(bnn.get_lr()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEACAYAAACUMoD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f449a7b5a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coeff_klrw = 1 / n_batches\n",
    "\n",
    "plt.plot(fs, 'r')\n",
    "plt.plot(qs*coeff_klrw, 'b')\n",
    "plt.plot(ps*coeff_klrw, 'g')\n",
    "plt.plot(ls, 'k')\n",
    "\n",
    "# plt.plot(fs[0:22], 'r')\n",
    "# plt.plot(qs[0:22], 'b')\n",
    "# plt.plot(ps[0:22], 'g')\n",
    "# plt.plot(ls[0:22], 'k')\n",
    "\n",
    "\n",
    "plt.legend(['f', 'q', 'p', 'l'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f449a7978d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(taccs, 'b')\n",
    "plt.plot(vaccs, 'r')\n",
    "\n",
    "plt.legend(['train_acc', 'valid_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer done\n",
      "layer done\n",
      "(10, ?, 10)\n"
     ]
    }
   ],
   "source": [
    "#bnn = bnn_model(shape, mu = 0.1, rho = 0.1, n_samples = 10, outact = tf.sigmoid, seed = 1234, lr = 1e-8)\n",
    "bnn = bnn_model([784, 50, 10], size_data = len(t_train), size_batch = batch_size, \\\n",
    "                mu = 0.02, rho = -2, n_samples = 10, outact = tf.sigmoid, seed = 1234, \\\n",
    "                lr = 1e-3, kl_reweight = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, ep 0, training accuracy 0.08\n",
      "f : 397671.15625, q : -334399.0625, p : -711578.5, l : 18939.7929688\n",
      "batch 0, ep 50, training accuracy 0.115\n",
      "f : 332071.0625, q : -334081.875, p : -659654.75, l : 8485.72949219\n",
      "valid accuracy 0.1058\n",
      "batch 1, ep 0, training accuracy 0.15\n",
      "f : 284405.59375, q : -329900.15625, p : -610027.75, l : 6489.69189453\n",
      "batch 1, ep 50, training accuracy 0.12\n",
      "f : 249911.296875, q : -322115.59375, p : -568315.0625, l : 5759.35058594\n",
      "valid accuracy 0.0904\n",
      "batch 2, ep 0, training accuracy 0.1\n",
      "f : 226866.5, q : -313039.71875, p : -533910.3125, l : 5718.3828125\n",
      "batch 2, ep 50, training accuracy 0.08\n",
      "f : 210229.015625, q : -303139.375, p : -507731.09375, l : 5401.47363281\n",
      "valid accuracy 0.1103\n",
      "batch 3, ep 0, training accuracy 0.17\n",
      "f : 196789.421875, q : -292977.40625, p : -485069.03125, l : 5530.83789062\n",
      "batch 3, ep 50, training accuracy 0.035\n",
      "f : 189630.46875, q : -283326.15625, p : -466063.4375, l : 5430.67822266\n",
      "valid accuracy 0.081\n",
      "batch 4, ep 0, training accuracy 0.165\n",
      "f : 181561.078125, q : -273229.875, p : -450070.0625, l : 5432.02148438\n",
      "batch 4, ep 50, training accuracy 0.08\n",
      "f : 176580.625, q : -264368.8125, p : -436533.78125, l : 5358.55859375\n",
      "valid accuracy 0.0928\n",
      "batch 5, ep 0, training accuracy 0.075\n",
      "f : 171692.078125, q : -259051.25, p : -424904.125, l : 5539.93847656\n",
      "batch 5, ep 50, training accuracy 0.115\n",
      "f : 167356.453125, q : -252111.3125, p : -413926.03125, l : 5506.80859375\n",
      "valid accuracy 0.1252\n",
      "batch 6, ep 0, training accuracy 0.09\n",
      "f : 163826.40625, q : -246521.4375, p : -404927.5, l : 5464.96582031\n",
      "batch 6, ep 50, training accuracy 0.095\n",
      "f : 160385.984375, q : -241635.4375, p : -396366.4375, l : 5355.74511719\n",
      "valid accuracy 0.102\n",
      "batch 7, ep 0, training accuracy 0.105\n",
      "f : 157790.515625, q : -237517.34375, p : -388698.625, l : 5669.0234375\n",
      "batch 7, ep 50, training accuracy 0.11\n",
      "f : 154237.1875, q : -233253.671875, p : -382939.3125, l : 5508.85888672\n",
      "valid accuracy 0.0759\n",
      "batch 8, ep 0, training accuracy 0.11\n",
      "f : 152285.75, q : -229504.015625, p : -377181.5625, l : 5449.36621094\n",
      "batch 8, ep 50, training accuracy 0.08\n",
      "f : 150526.109375, q : -226683.1875, p : -370800.4375, l : 5364.00634766\n",
      "valid accuracy 0.1053\n",
      "batch 9, ep 0, training accuracy 0.08\n",
      "f : 148994.8125, q : -223788.5625, p : -366404.96875, l : 5394.42333984\n",
      "batch 9, ep 50, training accuracy 0.135\n",
      "f : 146363.890625, q : -220750.625, p : -362685.28125, l : 5328.29101562\n",
      "valid accuracy 0.2134\n",
      "batch 10, ep 0, training accuracy 0.08\n",
      "f : 145182.390625, q : -218560.65625, p : -358989.9375, l : 5369.78369141\n",
      "batch 10, ep 50, training accuracy 0.165\n",
      "f : 143344.734375, q : -216667.34375, p : -354558.125, l : 5460.04541016\n",
      "valid accuracy 0.1319\n",
      "batch 11, ep 0, training accuracy 0.14\n",
      "f : 142213.890625, q : -215147.4375, p : -351341.0625, l : 5332.07519531\n",
      "batch 11, ep 50, training accuracy 0.125\n",
      "f : 141425.125, q : -213156.9375, p : -349555.625, l : 5365.08886719\n",
      "valid accuracy 0.0883\n",
      "batch 12, ep 0, training accuracy 0.14\n",
      "f : 140290.765625, q : -211031.625, p : -345003.1875, l : 5300.76123047\n",
      "batch 12, ep 50, training accuracy 0.085\n",
      "f : 138943.328125, q : -210809.65625, p : -343959.25, l : 5318.27539062\n",
      "valid accuracy 0.1508\n",
      "batch 13, ep 0, training accuracy 0.08\n",
      "f : 138207.5, q : -209483.375, p : -342191.28125, l : 5492.21777344\n",
      "batch 13, ep 50, training accuracy 0.13\n",
      "f : 136808.09375, q : -208908.109375, p : -341793.9375, l : 5263.39990234\n",
      "valid accuracy 0.1088\n",
      "batch 14, ep 0, training accuracy 0.15\n",
      "f : 136662.90625, q : -208877.34375, p : -341143.25, l : 5101.05273438\n",
      "batch 14, ep 50, training accuracy 0.17\n",
      "f : 136733.40625, q : -208958.0625, p : -341030.0, l : 5259.50097656\n",
      "valid accuracy 0.1384\n",
      "batch 15, ep 0, training accuracy 0.15\n",
      "f : 137019.46875, q : -208700.84375, p : -338914.03125, l : 5342.33886719\n",
      "batch 15, ep 50, training accuracy 0.15\n",
      "f : 136894.03125, q : -208767.09375, p : -339859.0625, l : 5066.68847656\n",
      "valid accuracy 0.0992\n",
      "batch 16, ep 0, training accuracy 0.105\n",
      "f : 136317.515625, q : -208136.234375, p : -339553.84375, l : 5215.50976562\n",
      "batch 16, ep 50, training accuracy 0.16\n",
      "f : 137091.671875, q : -208177.640625, p : -338691.125, l : 5174.17041016\n",
      "valid accuracy 0.1167\n",
      "batch 17, ep 0, training accuracy 0.16\n",
      "f : 136594.53125, q : -207252.3125, p : -339572.78125, l : 5143.47753906\n",
      "batch 17, ep 50, training accuracy 0.15\n",
      "f : 136808.25, q : -207381.9375, p : -339014.875, l : 5208.97851562\n",
      "valid accuracy 0.1393\n",
      "batch 18, ep 0, training accuracy 0.13\n",
      "f : 136692.8125, q : -208148.40625, p : -339274.6875, l : 5149.4921875\n",
      "batch 18, ep 50, training accuracy 0.135\n",
      "f : 135672.453125, q : -208038.625, p : -338262.875, l : 5455.66308594\n",
      "valid accuracy 0.1196\n",
      "batch 19, ep 0, training accuracy 0.14\n",
      "f : 136368.375, q : -207407.3125, p : -337094.15625, l : 5240.24804688\n",
      "batch 19, ep 50, training accuracy 0.155\n",
      "f : 135546.53125, q : -207344.015625, p : -338356.5625, l : 5285.32226562\n",
      "valid accuracy 0.1324\n",
      "batch 20, ep 0, training accuracy 0.105\n",
      "f : 135971.875, q : -206446.46875, p : -337520.375, l : 5185.79443359\n",
      "batch 20, ep 50, training accuracy 0.18\n",
      "f : 135399.8125, q : -206512.21875, p : -336622.5, l : 5302.72753906\n",
      "valid accuracy 0.1358\n",
      "batch 21, ep 0, training accuracy 0.09\n",
      "f : 135420.75, q : -205722.0625, p : -336394.5, l : 5461.08642578\n",
      "batch 21, ep 50, training accuracy 0.21\n",
      "f : 135286.6875, q : -206624.84375, p : -337213.75, l : 5227.72363281\n",
      "valid accuracy 0.1127\n",
      "batch 22, ep 0, training accuracy 0.13\n",
      "f : 135196.078125, q : -206534.625, p : -336259.6875, l : 5453.00195312\n",
      "batch 22, ep 50, training accuracy 0.19\n",
      "f : 134739.09375, q : -205958.5625, p : -336155.8125, l : 5308.76757812\n",
      "valid accuracy 0.1211\n",
      "batch 23, ep 0, training accuracy 0.155\n",
      "f : 135164.234375, q : -206656.71875, p : -336142.0, l : 5383.24658203\n",
      "batch 23, ep 50, training accuracy 0.165\n",
      "f : 135837.1875, q : -206783.578125, p : -337142.625, l : 5508.9921875\n",
      "valid accuracy 0.1304\n",
      "batch 24, ep 0, training accuracy 0.16\n",
      "f : 135336.0, q : -205566.5, p : -335905.75, l : 5245.38427734\n",
      "batch 24, ep 50, training accuracy 0.145\n",
      "f : 134722.578125, q : -206132.046875, p : -336991.1875, l : 5275.56201172\n",
      "valid accuracy 0.1307\n",
      "batch 25, ep 0, training accuracy 0.14\n",
      "f : 134522.671875, q : -206687.90625, p : -337909.625, l : 5186.31005859\n",
      "batch 25, ep 50, training accuracy 0.105\n",
      "f : 134694.0, q : -206571.71875, p : -336239.59375, l : 5353.48632812\n",
      "valid accuracy 0.1067\n",
      "batch 26, ep 0, training accuracy 0.185\n",
      "f : 135319.09375, q : -206978.6875, p : -336662.09375, l : 5327.55664062\n",
      "batch 26, ep 50, training accuracy 0.105\n",
      "f : 135172.34375, q : -206005.8125, p : -336780.21875, l : 5271.94921875\n",
      "valid accuracy 0.2089\n",
      "batch 27, ep 0, training accuracy 0.115\n",
      "f : 135148.875, q : -206243.40625, p : -336203.1875, l : 5331.34277344\n",
      "batch 27, ep 50, training accuracy 0.175\n",
      "f : 135538.515625, q : -204545.296875, p : -336731.03125, l : 5253.89306641\n",
      "valid accuracy 0.1382\n",
      "batch 28, ep 0, training accuracy 0.13\n",
      "f : 135254.484375, q : -206748.640625, p : -336782.25, l : 5301.31591797\n",
      "batch 28, ep 50, training accuracy 0.17\n",
      "f : 135424.59375, q : -206197.40625, p : -336286.90625, l : 5329.22900391\n",
      "valid accuracy 0.132\n",
      "batch 29, ep 0, training accuracy 0.16\n",
      "f : 134792.0625, q : -206530.09375, p : -336326.5625, l : 5305.22949219\n",
      "batch 29, ep 50, training accuracy 0.2\n",
      "f : 135447.203125, q : -206393.875, p : -336721.9375, l : 5332.89111328\n",
      "valid accuracy 0.1617\n",
      "batch 30, ep 0, training accuracy 0.165\n",
      "f : 135900.9375, q : -206032.25, p : -336445.1875, l : 5435.80615234\n",
      "batch 30, ep 50, training accuracy 0.155\n",
      "f : 135247.859375, q : -206206.015625, p : -336132.8125, l : 5452.98974609\n",
      "valid accuracy 0.1768\n",
      "batch 31, ep 0, training accuracy 0.145\n",
      "f : 135203.234375, q : -206889.59375, p : -336676.65625, l : 5252.59326172\n",
      "batch 31, ep 50, training accuracy 0.16\n",
      "f : 135221.078125, q : -206265.234375, p : -336320.875, l : 5094.02294922\n",
      "valid accuracy 0.1098\n",
      "batch 32, ep 0, training accuracy 0.17\n",
      "f : 134768.3125, q : -207269.40625, p : -335843.9375, l : 5167.63818359\n",
      "batch 32, ep 50, training accuracy 0.125\n",
      "f : 135401.59375, q : -205977.265625, p : -336569.75, l : 5424.98144531\n",
      "valid accuracy 0.0952\n",
      "batch 33, ep 0, training accuracy 0.095\n",
      "f : 134684.46875, q : -206164.03125, p : -336848.0, l : 5251.88378906\n",
      "batch 33, ep 50, training accuracy 0.195\n",
      "f : 135716.53125, q : -205941.84375, p : -336940.6875, l : 5230.62011719\n",
      "valid accuracy 0.1788\n",
      "batch 34, ep 0, training accuracy 0.11\n",
      "f : 135702.609375, q : -205591.578125, p : -336368.53125, l : 5272.73876953\n",
      "batch 34, ep 50, training accuracy 0.18\n",
      "f : 135047.78125, q : -206124.9375, p : -336398.125, l : 5276.0625\n",
      "valid accuracy 0.1647\n",
      "batch 35, ep 0, training accuracy 0.14\n",
      "f : 135189.375, q : -206101.234375, p : -336072.9375, l : 5223.64599609\n",
      "batch 35, ep 50, training accuracy 0.145\n",
      "f : 135307.734375, q : -206106.359375, p : -335867.3125, l : 5262.83398438\n",
      "valid accuracy 0.0925\n",
      "batch 36, ep 0, training accuracy 0.17\n",
      "f : 134668.609375, q : -206433.0625, p : -335508.84375, l : 5318.29638672\n",
      "batch 36, ep 50, training accuracy 0.145\n",
      "f : 135197.5, q : -206266.78125, p : -335763.625, l : 5392.37158203\n",
      "valid accuracy 0.171\n",
      "batch 37, ep 0, training accuracy 0.15\n",
      "f : 134964.0, q : -206176.65625, p : -336488.46875, l : 5389.56787109\n",
      "batch 37, ep 50, training accuracy 0.155\n",
      "f : 135487.109375, q : -206599.984375, p : -336242.53125, l : 5192.29345703\n",
      "valid accuracy 0.1906\n",
      "batch 38, ep 0, training accuracy 0.07\n",
      "f : 134648.21875, q : -206058.5, p : -336643.03125, l : 5380.32714844\n",
      "batch 38, ep 50, training accuracy 0.16\n",
      "f : 135705.6875, q : -206520.96875, p : -336234.625, l : 5233.30322266\n",
      "valid accuracy 0.1253\n",
      "batch 39, ep 0, training accuracy 0.13\n",
      "f : 134760.046875, q : -206676.46875, p : -336160.4375, l : 5335.40917969\n",
      "batch 39, ep 50, training accuracy 0.14\n",
      "f : 135050.203125, q : -206203.21875, p : -335230.59375, l : 5234.11376953\n",
      "valid accuracy 0.1198\n",
      "batch 40, ep 0, training accuracy 0.2\n",
      "f : 134915.28125, q : -206546.6875, p : -335929.65625, l : 5295.78466797\n",
      "batch 40, ep 50, training accuracy 0.19\n",
      "f : 135162.78125, q : -206610.65625, p : -336987.84375, l : 5306.47753906\n",
      "valid accuracy 0.1689\n",
      "batch 41, ep 0, training accuracy 0.13\n",
      "f : 135699.796875, q : -206675.3125, p : -336570.0, l : 5257.39306641\n",
      "batch 41, ep 50, training accuracy 0.115\n",
      "f : 134722.125, q : -206695.515625, p : -335877.125, l : 5373.06835938\n",
      "valid accuracy 0.1428\n",
      "batch 42, ep 0, training accuracy 0.125\n",
      "f : 134841.421875, q : -206241.484375, p : -336125.15625, l : 5188.60644531\n",
      "batch 42, ep 50, training accuracy 0.215\n",
      "f : 135401.875, q : -206595.375, p : -336582.65625, l : 5093.91113281\n",
      "valid accuracy 0.1277\n",
      "batch 43, ep 0, training accuracy 0.15\n",
      "f : 135163.6875, q : -205985.96875, p : -336166.625, l : 5323.79199219\n",
      "batch 43, ep 50, training accuracy 0.15\n",
      "f : 134598.515625, q : -206857.25, p : -336365.125, l : 5273.57910156\n",
      "valid accuracy 0.1448\n",
      "batch 44, ep 0, training accuracy 0.12\n",
      "f : 135842.0, q : -206040.40625, p : -337164.1875, l : 5365.48242188\n",
      "batch 44, ep 50, training accuracy 0.105\n",
      "f : 134860.96875, q : -206873.875, p : -335855.625, l : 5244.79638672\n",
      "valid accuracy 0.1508\n",
      "batch 45, ep 0, training accuracy 0.24\n",
      "f : 135542.609375, q : -207255.25, p : -336113.90625, l : 5390.16552734\n",
      "batch 45, ep 50, training accuracy 0.215\n",
      "f : 134907.125, q : -206670.21875, p : -335655.5, l : 5302.32519531\n",
      "valid accuracy 0.1601\n",
      "batch 46, ep 0, training accuracy 0.11\n",
      "f : 135071.015625, q : -205949.125, p : -336999.0, l : 5284.12304688\n",
      "batch 46, ep 50, training accuracy 0.21\n",
      "f : 135189.453125, q : -206233.40625, p : -335316.3125, l : 5293.78857422\n",
      "valid accuracy 0.1272\n",
      "batch 47, ep 0, training accuracy 0.155\n",
      "f : 135311.53125, q : -206080.515625, p : -336071.46875, l : 5251.67480469\n",
      "batch 47, ep 50, training accuracy 0.255\n",
      "f : 135377.78125, q : -205277.15625, p : -336320.25, l : 5353.38916016\n",
      "valid accuracy 0.2035\n",
      "batch 48, ep 0, training accuracy 0.19\n",
      "f : 134909.625, q : -206123.6875, p : -336479.78125, l : 5309.63720703\n",
      "batch 48, ep 50, training accuracy 0.165\n",
      "f : 134894.71875, q : -206765.421875, p : -335903.96875, l : 5289.18554688\n",
      "valid accuracy 0.1441\n",
      "batch 49, ep 0, training accuracy 0.115\n",
      "f : 135175.09375, q : -206340.125, p : -336929.3125, l : 5398.24560547\n",
      "batch 49, ep 50, training accuracy 0.195\n",
      "f : 135175.65625, q : -206797.84375, p : -336820.625, l : 5413.83837891\n",
      "valid accuracy 0.2011\n",
      "batch 50, ep 0, training accuracy 0.175\n",
      "f : 134987.5, q : -206137.671875, p : -336749.625, l : 5167.63476562\n",
      "batch 50, ep 50, training accuracy 0.185\n",
      "f : 135337.65625, q : -206539.296875, p : -336281.8125, l : 5348.00146484\n",
      "valid accuracy 0.2093\n",
      "batch 51, ep 0, training accuracy 0.145\n",
      "f : 134611.3125, q : -206229.0625, p : -335787.25, l : 5188.45068359\n",
      "batch 51, ep 50, training accuracy 0.11\n",
      "f : 134584.5, q : -206705.03125, p : -337479.90625, l : 5339.52685547\n",
      "valid accuracy 0.2068\n",
      "batch 52, ep 0, training accuracy 0.14\n",
      "f : 135590.234375, q : -206585.21875, p : -336654.34375, l : 5445.140625\n",
      "batch 52, ep 50, training accuracy 0.17\n",
      "f : 135345.171875, q : -206723.625, p : -336835.65625, l : 5219.36914062\n",
      "valid accuracy 0.1506\n",
      "batch 53, ep 0, training accuracy 0.17\n",
      "f : 135455.390625, q : -206391.265625, p : -336667.03125, l : 5348.0703125\n",
      "batch 53, ep 50, training accuracy 0.11\n",
      "f : 135200.21875, q : -206233.3125, p : -336035.5, l : 5272.70410156\n",
      "valid accuracy 0.1175\n",
      "batch 54, ep 0, training accuracy 0.16\n",
      "f : 134936.296875, q : -206675.875, p : -336350.3125, l : 5417.86328125\n",
      "batch 54, ep 50, training accuracy 0.205\n",
      "f : 135366.53125, q : -206213.921875, p : -337126.28125, l : 5229.109375\n",
      "valid accuracy 0.1407\n",
      "batch 55, ep 0, training accuracy 0.18\n",
      "f : 135291.109375, q : -206202.828125, p : -336658.03125, l : 5254.83984375\n",
      "batch 55, ep 50, training accuracy 0.17\n",
      "f : 135630.140625, q : -206511.59375, p : -337212.25, l : 5248.60107422\n",
      "valid accuracy 0.1912\n",
      "batch 56, ep 0, training accuracy 0.105\n",
      "f : 135182.796875, q : -206122.421875, p : -336436.0625, l : 5269.44921875\n",
      "batch 56, ep 50, training accuracy 0.13\n",
      "f : 135080.90625, q : -206480.65625, p : -337738.75, l : 5142.859375\n",
      "valid accuracy 0.1352\n",
      "batch 57, ep 0, training accuracy 0.18\n",
      "f : 134987.21875, q : -207223.8125, p : -336098.125, l : 5323.84033203\n",
      "batch 57, ep 50, training accuracy 0.095\n",
      "f : 135382.0625, q : -205942.828125, p : -336369.78125, l : 5256.93652344\n",
      "valid accuracy 0.1892\n",
      "batch 58, ep 0, training accuracy 0.155\n",
      "f : 135117.15625, q : -206517.6875, p : -335220.3125, l : 5541.53515625\n",
      "batch 58, ep 50, training accuracy 0.15\n",
      "f : 135058.0625, q : -206295.96875, p : -336059.8125, l : 5442.19482422\n",
      "valid accuracy 0.1817\n",
      "batch 59, ep 0, training accuracy 0.17\n",
      "f : 135255.46875, q : -206808.609375, p : -335762.875, l : 5307.01074219\n",
      "batch 59, ep 50, training accuracy 0.175\n",
      "f : 135708.75, q : -206318.515625, p : -336675.5625, l : 5308.32910156\n",
      "valid accuracy 0.1225\n",
      "batch 60, ep 0, training accuracy 0.165\n",
      "f : 134855.21875, q : -206852.3125, p : -336079.59375, l : 5148.94677734\n",
      "batch 60, ep 50, training accuracy 0.125\n",
      "f : 135053.125, q : -206601.75, p : -336706.0, l : 5552.98632812\n",
      "valid accuracy 0.1793\n",
      "batch 61, ep 0, training accuracy 0.11\n",
      "f : 135012.78125, q : -206322.453125, p : -335185.84375, l : 5099.48486328\n",
      "batch 61, ep 50, training accuracy 0.14\n",
      "f : 134543.28125, q : -205570.40625, p : -335918.0625, l : 5249.79492188\n",
      "valid accuracy 0.1174\n",
      "batch 62, ep 0, training accuracy 0.115\n",
      "f : 134766.34375, q : -207172.3125, p : -335999.84375, l : 5403.203125\n",
      "batch 62, ep 50, training accuracy 0.18\n",
      "f : 135233.765625, q : -206174.484375, p : -337487.1875, l : 5402.89257812\n",
      "valid accuracy 0.079\n",
      "batch 63, ep 0, training accuracy 0.205\n",
      "f : 135186.875, q : -205230.171875, p : -336376.4375, l : 5254.96972656\n",
      "batch 63, ep 50, training accuracy 0.15\n",
      "f : 135629.0, q : -207096.84375, p : -337085.4375, l : 5263.35253906\n",
      "valid accuracy 0.0916\n",
      "batch 64, ep 0, training accuracy 0.14\n",
      "f : 135599.59375, q : -206105.34375, p : -336319.53125, l : 5444.55566406\n",
      "batch 64, ep 50, training accuracy 0.13\n",
      "f : 135357.15625, q : -206702.5, p : -336337.5625, l : 5202.15039062\n",
      "valid accuracy 0.1285\n",
      "batch 65, ep 0, training accuracy 0.11\n",
      "f : 134880.203125, q : -206340.09375, p : -336158.28125, l : 5119.36914062\n",
      "batch 65, ep 50, training accuracy 0.15\n",
      "f : 134651.390625, q : -207069.15625, p : -336461.15625, l : 5347.91796875\n",
      "valid accuracy 0.2073\n",
      "batch 66, ep 0, training accuracy 0.12\n",
      "f : 134790.1875, q : -205859.1875, p : -336613.6875, l : 5360.26855469\n",
      "batch 66, ep 50, training accuracy 0.14\n",
      "f : 135464.5625, q : -206327.734375, p : -336128.59375, l : 5145.09326172\n",
      "valid accuracy 0.1691\n",
      "batch 67, ep 0, training accuracy 0.2\n",
      "f : 135230.390625, q : -206063.234375, p : -337766.875, l : 5409.19287109\n",
      "batch 67, ep 50, training accuracy 0.115\n",
      "f : 135372.078125, q : -206896.25, p : -336605.25, l : 5167.94091797\n",
      "valid accuracy 0.1869\n",
      "batch 68, ep 0, training accuracy 0.105\n",
      "f : 135121.015625, q : -205847.96875, p : -336053.25, l : 5269.66796875\n",
      "batch 68, ep 50, training accuracy 0.19\n",
      "f : 135668.109375, q : -207264.46875, p : -336304.71875, l : 5515.71728516\n",
      "valid accuracy 0.1537\n",
      "batch 69, ep 0, training accuracy 0.08\n",
      "f : 135447.96875, q : -205776.71875, p : -335111.21875, l : 5453.68994141\n",
      "batch 69, ep 50, training accuracy 0.13\n",
      "f : 134921.125, q : -205778.796875, p : -336900.9375, l : 5340.24951172\n",
      "valid accuracy 0.1712\n",
      "batch 70, ep 0, training accuracy 0.125\n",
      "f : 135043.296875, q : -206816.09375, p : -335824.8125, l : 5128.26367188\n",
      "batch 70, ep 50, training accuracy 0.105\n",
      "f : 135615.59375, q : -206499.40625, p : -338085.375, l : 5360.38037109\n",
      "valid accuracy 0.1145\n",
      "batch 71, ep 0, training accuracy 0.185\n",
      "f : 135414.59375, q : -206692.09375, p : -336071.5625, l : 5199.88769531\n",
      "batch 71, ep 50, training accuracy 0.15\n",
      "f : 134752.078125, q : -206270.875, p : -336207.1875, l : 5152.23779297\n",
      "valid accuracy 0.1303\n",
      "batch 72, ep 0, training accuracy 0.145\n",
      "f : 135043.296875, q : -206607.734375, p : -336312.21875, l : 5311.80810547\n",
      "batch 72, ep 50, training accuracy 0.13\n",
      "f : 135540.0, q : -206201.53125, p : -336009.28125, l : 5341.21728516\n",
      "valid accuracy 0.1759\n",
      "batch 73, ep 0, training accuracy 0.11\n",
      "f : 135807.453125, q : -206225.5, p : -335823.5, l : 5421.35107422\n",
      "batch 73, ep 50, training accuracy 0.125\n",
      "f : 135122.90625, q : -206816.96875, p : -335587.59375, l : 5406.06640625\n",
      "valid accuracy 0.1577\n",
      "batch 74, ep 0, training accuracy 0.175\n",
      "f : 134823.53125, q : -207027.3125, p : -335923.9375, l : 5357.82714844\n",
      "batch 74, ep 50, training accuracy 0.16\n",
      "f : 135714.265625, q : -206951.078125, p : -336282.4375, l : 5404.46679688\n",
      "valid accuracy 0.1899\n",
      "batch 75, ep 0, training accuracy 0.155\n",
      "f : 135305.03125, q : -206709.953125, p : -336845.8125, l : 5218.66650391\n",
      "batch 75, ep 50, training accuracy 0.19\n",
      "f : 135450.234375, q : -207083.625, p : -336821.34375, l : 5268.19335938\n",
      "valid accuracy 0.1352\n",
      "batch 76, ep 0, training accuracy 0.21\n",
      "f : 134896.75, q : -205993.34375, p : -335897.875, l : 5275.23583984\n",
      "batch 76, ep 50, training accuracy 0.185\n",
      "f : 135435.984375, q : -206384.265625, p : -336074.75, l : 5225.15429688\n",
      "valid accuracy 0.1331\n",
      "batch 77, ep 0, training accuracy 0.175\n",
      "f : 135261.921875, q : -206232.90625, p : -336097.875, l : 5198.52587891\n",
      "batch 77, ep 50, training accuracy 0.15\n",
      "f : 135089.28125, q : -206893.46875, p : -336566.625, l : 5310.38476562\n",
      "valid accuracy 0.1119\n",
      "batch 78, ep 0, training accuracy 0.12\n",
      "f : 135563.96875, q : -205960.5625, p : -336530.625, l : 5341.88867188\n",
      "batch 78, ep 50, training accuracy 0.09\n",
      "f : 135210.84375, q : -206666.234375, p : -336440.125, l : 5090.34179688\n",
      "valid accuracy 0.1393\n",
      "batch 79, ep 0, training accuracy 0.14\n",
      "f : 135248.265625, q : -206630.84375, p : -336811.875, l : 5361.96240234\n",
      "batch 79, ep 50, training accuracy 0.18\n",
      "f : 135249.0625, q : -207550.546875, p : -336847.9375, l : 5275.79345703\n",
      "valid accuracy 0.1071\n",
      "batch 80, ep 0, training accuracy 0.16\n",
      "f : 134978.84375, q : -206102.203125, p : -336197.21875, l : 5338.66308594\n",
      "batch 80, ep 50, training accuracy 0.07\n",
      "f : 135498.53125, q : -205929.9375, p : -336768.59375, l : 5299.65917969\n",
      "valid accuracy 0.1493\n",
      "batch 81, ep 0, training accuracy 0.225\n",
      "f : 134528.0625, q : -205399.03125, p : -336515.875, l : 5101.61572266\n",
      "batch 81, ep 50, training accuracy 0.2\n",
      "f : 135182.3125, q : -206284.046875, p : -335820.34375, l : 5304.81054688\n",
      "valid accuracy 0.1279\n",
      "batch 82, ep 0, training accuracy 0.16\n",
      "f : 134790.9375, q : -206989.140625, p : -336864.03125, l : 5393.38623047\n",
      "batch 82, ep 50, training accuracy 0.125\n",
      "f : 134842.4375, q : -206398.1875, p : -336322.125, l : 5356.23339844\n",
      "valid accuracy 0.1822\n",
      "batch 83, ep 0, training accuracy 0.195\n",
      "f : 135100.625, q : -206629.296875, p : -336182.28125, l : 5235.85888672\n",
      "batch 83, ep 50, training accuracy 0.165\n",
      "f : 134950.1875, q : -206815.53125, p : -336563.625, l : 5356.02099609\n",
      "valid accuracy 0.1577\n",
      "batch 84, ep 0, training accuracy 0.135\n",
      "f : 134857.125, q : -206129.34375, p : -336296.0, l : 5143.80517578\n",
      "batch 84, ep 50, training accuracy 0.13\n",
      "f : 135252.921875, q : -206471.34375, p : -336422.0, l : 5346.46777344\n",
      "valid accuracy 0.1865\n",
      "batch 85, ep 0, training accuracy 0.17\n",
      "f : 135821.4375, q : -206531.328125, p : -336322.1875, l : 5368.89746094\n",
      "batch 85, ep 50, training accuracy 0.175\n",
      "f : 134813.65625, q : -206197.171875, p : -336401.40625, l : 5341.03564453\n",
      "valid accuracy 0.1217\n",
      "batch 86, ep 0, training accuracy 0.205\n",
      "f : 135529.96875, q : -206263.234375, p : -336842.0625, l : 5260.77636719\n",
      "batch 86, ep 50, training accuracy 0.145\n",
      "f : 135696.5, q : -206489.6875, p : -336805.78125, l : 5441.66503906\n",
      "valid accuracy 0.1686\n",
      "batch 87, ep 0, training accuracy 0.135\n",
      "f : 135431.265625, q : -206356.21875, p : -336549.25, l : 5280.43945312\n",
      "batch 87, ep 50, training accuracy 0.13\n",
      "f : 134771.6875, q : -205844.3125, p : -335540.875, l : 5095.75683594\n",
      "valid accuracy 0.135\n",
      "batch 88, ep 0, training accuracy 0.21\n",
      "f : 135364.46875, q : -205682.6875, p : -336174.875, l : 5388.44775391\n",
      "batch 88, ep 50, training accuracy 0.18\n",
      "f : 135236.671875, q : -205929.921875, p : -335839.75, l : 5344.54296875\n",
      "valid accuracy 0.1914\n",
      "batch 89, ep 0, training accuracy 0.085\n",
      "f : 135427.3125, q : -206901.0, p : -336758.6875, l : 5285.08886719\n",
      "batch 89, ep 50, training accuracy 0.145\n",
      "f : 135427.984375, q : -205881.65625, p : -336731.5, l : 5303.09570312\n",
      "valid accuracy 0.1635\n",
      "batch 90, ep 0, training accuracy 0.17\n",
      "f : 134942.3125, q : -207559.015625, p : -336776.78125, l : 5257.171875\n",
      "batch 90, ep 50, training accuracy 0.15\n",
      "f : 135082.90625, q : -206700.046875, p : -335992.625, l : 5211.26025391\n",
      "valid accuracy 0.1531\n",
      "batch 91, ep 0, training accuracy 0.08\n",
      "f : 134819.53125, q : -206569.78125, p : -336900.625, l : 5364.68652344\n",
      "batch 91, ep 50, training accuracy 0.135\n",
      "f : 135181.359375, q : -206214.5, p : -337603.125, l : 5355.82519531\n",
      "valid accuracy 0.1707\n",
      "batch 92, ep 0, training accuracy 0.245\n",
      "f : 135278.328125, q : -205860.53125, p : -335679.375, l : 5194.52880859\n",
      "batch 92, ep 50, training accuracy 0.115\n",
      "f : 134954.375, q : -206108.5625, p : -335736.59375, l : 5222.33886719\n",
      "valid accuracy 0.1521\n",
      "batch 93, ep 0, training accuracy 0.195\n",
      "f : 135510.265625, q : -205826.59375, p : -336325.34375, l : 5230.94970703\n",
      "batch 93, ep 50, training accuracy 0.185\n",
      "f : 135951.125, q : -206320.21875, p : -336811.09375, l : 5365.50683594\n",
      "valid accuracy 0.1187\n",
      "batch 94, ep 0, training accuracy 0.13\n",
      "f : 134847.78125, q : -206400.515625, p : -336001.03125, l : 5252.34570312\n",
      "batch 94, ep 50, training accuracy 0.185\n",
      "f : 134875.703125, q : -206497.640625, p : -336026.375, l : 5358.66601562\n",
      "valid accuracy 0.0845\n",
      "batch 95, ep 0, training accuracy 0.12\n",
      "f : 135361.28125, q : -205303.90625, p : -335994.0, l : 5297.78369141\n",
      "batch 95, ep 50, training accuracy 0.08\n",
      "f : 134827.4375, q : -206245.59375, p : -336202.84375, l : 5210.44628906\n",
      "valid accuracy 0.1689\n",
      "batch 96, ep 0, training accuracy 0.06\n",
      "f : 135574.5625, q : -205705.84375, p : -335544.5, l : 5220.01513672\n",
      "batch 96, ep 50, training accuracy 0.185\n",
      "f : 135201.046875, q : -206561.640625, p : -335781.0625, l : 5281.78466797\n",
      "valid accuracy 0.1852\n",
      "batch 97, ep 0, training accuracy 0.105\n",
      "f : 134950.640625, q : -206080.703125, p : -335918.5, l : 5266.51611328\n",
      "batch 97, ep 50, training accuracy 0.12\n",
      "f : 135295.515625, q : -205626.71875, p : -336701.6875, l : 5196.97167969\n",
      "valid accuracy 0.1536\n",
      "batch 98, ep 0, training accuracy 0.09\n",
      "f : 134813.734375, q : -206417.875, p : -336222.8125, l : 5293.16845703\n",
      "batch 98, ep 50, training accuracy 0.215\n",
      "f : 135375.59375, q : -206582.859375, p : -336633.375, l : 5287.78222656\n",
      "valid accuracy 0.1893\n",
      "batch 99, ep 0, training accuracy 0.11\n",
      "f : 134969.140625, q : -205607.328125, p : -336285.75, l : 5310.05859375\n",
      "batch 99, ep 50, training accuracy 0.13\n",
      "f : 135114.390625, q : -206720.6875, p : -336122.8125, l : 5331.18261719\n",
      "valid accuracy 0.1775\n",
      "batch 100, ep 0, training accuracy 0.135\n",
      "f : 135168.15625, q : -206618.75, p : -336937.1875, l : 5447.25292969\n",
      "batch 100, ep 50, training accuracy 0.2\n",
      "f : 135433.3125, q : -206721.5625, p : -337108.5, l : 5468.28564453\n",
      "valid accuracy 0.144\n",
      "batch 101, ep 0, training accuracy 0.17\n",
      "f : 135695.359375, q : -206409.859375, p : -336559.78125, l : 5260.33203125\n",
      "batch 101, ep 50, training accuracy 0.175\n",
      "f : 135279.71875, q : -206204.71875, p : -336784.875, l : 5185.36035156\n",
      "valid accuracy 0.1195\n",
      "batch 102, ep 0, training accuracy 0.195\n",
      "f : 134864.03125, q : -205993.125, p : -336419.09375, l : 5293.52734375\n",
      "batch 102, ep 50, training accuracy 0.07\n",
      "f : 135064.546875, q : -206606.546875, p : -336603.5625, l : 5464.18115234\n",
      "valid accuracy 0.1755\n",
      "batch 103, ep 0, training accuracy 0.125\n",
      "f : 135127.78125, q : -206268.796875, p : -336860.46875, l : 5401.41894531\n",
      "batch 103, ep 50, training accuracy 0.25\n",
      "f : 135027.796875, q : -206685.609375, p : -336391.8125, l : 5387.76611328\n",
      "valid accuracy 0.1993\n",
      "batch 104, ep 0, training accuracy 0.185\n",
      "f : 135419.546875, q : -206488.3125, p : -337124.28125, l : 5189.17626953\n",
      "batch 104, ep 50, training accuracy 0.225\n",
      "f : 135303.71875, q : -206716.40625, p : -335704.4375, l : 5404.15722656\n",
      "valid accuracy 0.1163\n",
      "batch 105, ep 0, training accuracy 0.215\n",
      "f : 135174.90625, q : -206191.84375, p : -336972.875, l : 5324.69335938\n",
      "batch 105, ep 50, training accuracy 0.24\n",
      "f : 135105.171875, q : -206087.4375, p : -336757.34375, l : 5316.42041016\n",
      "valid accuracy 0.123\n",
      "batch 106, ep 0, training accuracy 0.14\n",
      "f : 134541.359375, q : -206170.75, p : -336072.25, l : 5387.40087891\n",
      "batch 106, ep 50, training accuracy 0.12\n",
      "f : 135176.84375, q : -206843.96875, p : -336655.25, l : 5205.39550781\n",
      "valid accuracy 0.17\n",
      "batch 107, ep 0, training accuracy 0.125\n",
      "f : 134905.78125, q : -205621.875, p : -336677.28125, l : 5257.68994141\n",
      "batch 107, ep 50, training accuracy 0.19\n",
      "f : 135212.828125, q : -205838.578125, p : -336273.625, l : 5485.27832031\n",
      "valid accuracy 0.1568\n",
      "batch 108, ep 0, training accuracy 0.195\n",
      "f : 134800.09375, q : -206772.125, p : -336030.125, l : 5298.76855469\n",
      "batch 108, ep 50, training accuracy 0.19\n",
      "f : 135359.359375, q : -205817.53125, p : -336164.5625, l : 5328.24853516\n",
      "valid accuracy 0.108\n",
      "batch 109, ep 0, training accuracy 0.13\n",
      "f : 135220.34375, q : -205470.78125, p : -336338.0625, l : 5304.36621094\n",
      "batch 109, ep 50, training accuracy 0.145\n",
      "f : 135122.46875, q : -206021.8125, p : -336910.96875, l : 5073.97949219\n",
      "valid accuracy 0.1613\n",
      "batch 110, ep 0, training accuracy 0.215\n",
      "f : 135805.796875, q : -206221.34375, p : -336425.25, l : 5589.50341797\n",
      "batch 110, ep 50, training accuracy 0.095\n",
      "f : 136192.578125, q : -206563.28125, p : -334923.5, l : 5574.58642578\n",
      "valid accuracy 0.1464\n",
      "batch 111, ep 0, training accuracy 0.12\n",
      "f : 135451.84375, q : -206822.15625, p : -336276.5, l : 5215.19628906\n",
      "batch 111, ep 50, training accuracy 0.125\n",
      "f : 135807.890625, q : -206275.4375, p : -337008.78125, l : 5256.05175781\n",
      "valid accuracy 0.1767\n",
      "batch 112, ep 0, training accuracy 0.1\n",
      "f : 134995.40625, q : -207710.9375, p : -336056.4375, l : 5263.375\n",
      "batch 112, ep 50, training accuracy 0.195\n",
      "f : 134830.71875, q : -206689.65625, p : -336086.375, l : 5480.87109375\n",
      "valid accuracy 0.1834\n",
      "batch 113, ep 0, training accuracy 0.13\n",
      "f : 135016.65625, q : -206853.609375, p : -336079.4375, l : 5327.34179688\n",
      "batch 113, ep 50, training accuracy 0.175\n",
      "f : 135136.90625, q : -206229.34375, p : -335822.59375, l : 5244.33447266\n",
      "valid accuracy 0.1851\n",
      "batch 114, ep 0, training accuracy 0.235\n",
      "f : 135516.484375, q : -206409.71875, p : -336590.875, l : 5104.65478516\n",
      "batch 114, ep 50, training accuracy 0.145\n",
      "f : 135200.109375, q : -206363.171875, p : -336604.6875, l : 5050.36962891\n",
      "valid accuracy 0.1835\n",
      "batch 115, ep 0, training accuracy 0.215\n",
      "f : 135059.546875, q : -206881.875, p : -336814.875, l : 5285.75683594\n",
      "batch 115, ep 50, training accuracy 0.17\n",
      "f : 135384.171875, q : -206595.765625, p : -336394.5625, l : 5173.87255859\n",
      "valid accuracy 0.1379\n",
      "batch 116, ep 0, training accuracy 0.155\n",
      "f : 135481.25, q : -205711.109375, p : -336104.9375, l : 5209.54150391\n",
      "batch 116, ep 50, training accuracy 0.135\n",
      "f : 135266.359375, q : -206577.03125, p : -335785.375, l : 5203.1171875\n",
      "valid accuracy 0.1427\n",
      "batch 117, ep 0, training accuracy 0.085\n",
      "f : 135029.5, q : -206863.453125, p : -336597.75, l : 5310.22460938\n",
      "batch 117, ep 50, training accuracy 0.135\n",
      "f : 135026.234375, q : -206221.84375, p : -336258.59375, l : 5429.18505859\n",
      "valid accuracy 0.1272\n",
      "batch 118, ep 0, training accuracy 0.095\n",
      "f : 135083.984375, q : -207213.578125, p : -335685.3125, l : 5351.40869141\n",
      "batch 118, ep 50, training accuracy 0.12\n",
      "f : 134562.78125, q : -206156.0625, p : -335479.9375, l : 5428.765625\n",
      "valid accuracy 0.1254\n",
      "batch 119, ep 0, training accuracy 0.1\n",
      "f : 135812.59375, q : -206147.5, p : -336068.96875, l : 5262.77246094\n",
      "batch 119, ep 50, training accuracy 0.12\n",
      "f : 134878.296875, q : -207154.234375, p : -336496.28125, l : 5166.46875\n",
      "valid accuracy 0.1835\n",
      "batch 120, ep 0, training accuracy 0.165\n",
      "f : 135325.40625, q : -207217.21875, p : -336228.3125, l : 5189.64160156\n",
      "batch 120, ep 50, training accuracy 0.135\n",
      "f : 135098.796875, q : -206865.9375, p : -336861.25, l : 5213.57421875\n",
      "valid accuracy 0.1786\n",
      "batch 121, ep 0, training accuracy 0.115\n",
      "f : 135211.40625, q : -206216.5, p : -336376.90625, l : 5244.06347656\n",
      "batch 121, ep 50, training accuracy 0.155\n",
      "f : 135114.140625, q : -206772.75, p : -336179.5625, l : 5319.85058594\n",
      "valid accuracy 0.1856\n",
      "batch 122, ep 0, training accuracy 0.26\n",
      "f : 134653.890625, q : -206773.328125, p : -336327.25, l : 5352.97167969\n",
      "batch 122, ep 50, training accuracy 0.18\n",
      "f : 135624.921875, q : -206567.65625, p : -336086.25, l : 5457.45117188\n",
      "valid accuracy 0.1381\n",
      "batch 123, ep 0, training accuracy 0.12\n",
      "f : 135047.484375, q : -206694.515625, p : -336301.1875, l : 5348.9609375\n",
      "batch 123, ep 50, training accuracy 0.125\n",
      "f : 135295.015625, q : -206352.4375, p : -336462.6875, l : 5087.89746094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f85c58c9df87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[0mbnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mv_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fqpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/bayes_nn_shson/bnn_shson.pyc\u001b[0m in \u001b[0;36mget_fqpl\u001b[1;34m(self, feed)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_fqpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_q_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_p_pri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloglike\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m     \"\"\"\n\u001b[1;32m--> 559\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3654\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3655\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3656\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "n_epochs = 200\n",
    "n_batches = len(t_train) / batch_size\n",
    "\n",
    "fs = list()\n",
    "qs = list()\n",
    "ps = list()\n",
    "ls = list()\n",
    "taccs = list()\n",
    "vaccs = list()\n",
    "for i in range(n_batches):\n",
    "    \n",
    "    bnn.reset_lr()\n",
    "    \n",
    "    for ep in range(n_epochs):\n",
    "        \n",
    "        feed = {bnn.x: x_train[i*batch_size:(i+1)*batch_size], \\\n",
    "                bnn.t: t_train[i*batch_size:(i+1)*batch_size]}\n",
    "        \n",
    "        v_f, v_q, v_p, v_l = bnn.get_fqpl(feed)\n",
    "        fs.append(v_f), qs.append(v_q), ps.append(v_p), ls.append(v_l)\n",
    "\n",
    "        if ep > 50 and np.mean(fs[-50:-25]) < np.mean(fs[-25:]):\n",
    "            last_lr = bnn.get_lr()\n",
    "            bnn.decay_lr()\n",
    "            #print(\"=== learning rate decayed ===\")\n",
    "            if bnn.get_lr == last_lr:\n",
    "                print(\"=== cannot decay more. stop learning this batch ===\")\n",
    "                break\n",
    "            #print (\"--- learning rate decayed ---\")\n",
    "            #print bnn.get_lr()\n",
    "\n",
    "        if ep % 50 == 0:\n",
    "            train_accuracy = bnn.validate(feed)\n",
    "\n",
    "            print(\"batch %d, ep %d, training accuracy %g\"%(i, ep, train_accuracy))\n",
    "            print(\"f : {}, q : {}, p : {}, l : {}\".format(v_f, v_q, v_p, v_l))\n",
    "\n",
    "        bnn.train(feed)\n",
    "        \n",
    "    vacc = bnn.validate({bnn.x: x_valid, bnn.t: t_valid})\n",
    "    vaccs.append(vacc)\n",
    "    taccs.append(train_accuracy)\n",
    "    print(\"valid accuracy %g\"%vacc)\n",
    "    \n",
    "#     if i > 10 and np.mean(vaccs[-10:-5]) < np.mean(vaccs[-5:]):\n",
    "#         bnn.decay_lr()\n",
    "    \n",
    "    bnn.update_prior()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fs, 'r')\n",
    "plt.plot(qs, 'b')\n",
    "plt.plot(ps, 'g')\n",
    "plt.plot(ls, 'k')\n",
    "\n",
    "# plt.plot(fs[0:22], 'r')\n",
    "# plt.plot(qs[0:22], 'b')\n",
    "# plt.plot(ps[0:22], 'g')\n",
    "# plt.plot(ls[0:22], 'k')\n",
    "\n",
    "plt.legend(['f', 'q', 'p', 'l'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(taccs, 'b')\n",
    "plt.plot(vaccs, 'r')\n",
    "\n",
    "plt.legend(['train_acc', 'valid_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codes used for testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.range(300)\n",
    "aa = tf.reshape(a, [30, 10])\n",
    "aaa = tf.tile(tf.expand_dims(aa, 0), [10, 1, 1])\n",
    "\n",
    "b = tf.truncated_normal([30, 10], stddev = 0.2)\n",
    "\n",
    "argmaxbs = tf.argmax(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(30), Dimension(10)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2, ..., 297, 298, 299], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(aaa, [-1]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298   5  11  27  32  43  57\n",
      "  63  76  84  97 105 118 123 138 142 158 164 172 182 193 201 215 227 237\n",
      " 248 258 261 271 281 298   5  11  27  32  43  57  63  76  84  97 105 118\n",
      " 123 138 142 158 164 172 182 193 201 215 227 237 248 258 261 271 281 298\n",
      "   5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298   5  11  27  32  43  57\n",
      "  63  76  84  97 105 118 123 138 142 158 164 172 182 193 201 215 227 237\n",
      " 248 258 261 271 281 298   5  11  27  32  43  57  63  76  84  97 105 118\n",
      " 123 138 142 158 164 172 182 193 201 215 227 237 248 258 261 271 281 298\n",
      "   5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298   5  11  27  32  43  57\n",
      "  63  76  84  97 105 118 123 138 142 158 164 172 182 193 201 215 227 237\n",
      " 248 258 261 271 281 298   5  11  27  32  43  57  63  76  84  97 105 118\n",
      " 123 138 142 158 164 172 182 193 201 215 227 237 248 258 261 271 281 298\n",
      "   5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298]\n",
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 5, 3, 8, 6, 1, 0, 7, 0, 3, 7, 8, 0, 4, 5, 8, 8, 0, 2, 6, 7, 7,\n",
       "       0, 3, 7, 8, 3, 8, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = tf.range(30).eval()\n",
    "br = ar * 10 + argmaxbs.eval()\n",
    "cr = tf.tile(br, [10])\n",
    "\n",
    "result = tf.gather(tf.reshape(aaa, [-1]), cr).eval()\n",
    "print result\n",
    "print result.shape\n",
    "argmaxbs.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(a).eval()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519]]\n",
      "[[-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]]\n",
      "[[[-0.04836457  0.11699509 -0.10365508]\n",
      "  [-0.11315618 -0.14960578 -0.13785519]]\n",
      "\n",
      " [[-0.04836457  0.11699509 -0.10365508]\n",
      "  [-0.11315618 -0.14960578 -0.13785519]]]\n",
      "[[ 0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519]]\n",
      "[[ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]]\n",
      "[[ 0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519]]\n",
      "[[ 0.04836457  0.11315618]\n",
      " [-0.11699509  0.14960578]\n",
      " [ 0.10365508  0.13785519]]\n"
     ]
    }
   ],
   "source": [
    "with sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print a.eval()\n",
    "    print tf.tile(a, [6, 3]).eval()\n",
    "    print tf.tile(tf.expand_dims(a, 0), [2, 1, 1]).eval()\n",
    "    a = a * b\n",
    "    print a.eval()\n",
    "    print tf.tile(a, [6, 3]).eval()\n",
    "    print tf.transpose(a, [0, 1]).eval()\n",
    "    print tf.transpose(a, [1, 0]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.11307557  2.01307557]\n",
      "[ 2.11307557  2.01307557]\n"
     ]
    }
   ],
   "source": [
    "t1 = np.array([[0.1, 0.2, 0.3, 0.4, 0., 0., 0., 0., 0., 0.], [0.1, 0.2, 0.3, 0.4, 0., 0., 0., 0., 0., 0.]])\n",
    "t2 = np.array([[0., 0., 1, 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1, 0., 0., 0., 0., 0., 0.]])\n",
    "print tf.nn.softmax_cross_entropy_with_logits(t1, t2).eval()\n",
    "print tf.nn.softmax_cross_entropy_with_logits(t1, t2).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2039728"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.log(0.3).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]\n",
      "\n",
      " [[1]\n",
      "  [2]\n",
      "  [3]\n",
      "  [4]\n",
      "  [5]]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "t3 = tf.tile(tf.expand_dims(np.array([[1], [2], [3], [4], [5]]), 0), [10, 1, 1])\n",
    "print t3.eval()\n",
    "t4 = tf.reshape(t3, [-1, 1])\n",
    "print t4.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976630367\n"
     ]
    }
   ],
   "source": [
    "tsh = tf.shape(t3)\n",
    "tf.shape(t3).eval()\n",
    "print tsh[-1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
