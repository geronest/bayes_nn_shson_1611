{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os, sys, time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from bnn_shson import *\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.InteractiveSession()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer done\n",
      "(10, ?, 10)\n",
      "(10,)\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#bnn = bnn_model(shape, mu = 0.1, rho = 0.1, n_samples = 10, outact = tf.sigmoid, seed = 1234, lr = 1e-8)\n",
    "bnn = bnn_model([784, 50, 10], size_batch = batch_size, mu = 0.02, rho = -1, n_samples = 10, outact = tf.sigmoid, seed = 1234, lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14\n",
      "f : 28586.7324219, q : -77593.1796875, p : -106281.96875, l : -546.624633789\n",
      "step 100, training accuracy 0.26\n",
      "f : 13658.5136719, q : -57237.8164062, p : -70348.3984375, l : -335.87286377\n",
      "step 200, training accuracy 0.06\n",
      "f : 8667.45507812, q : -43618.0625, p : -52058.046875, l : -292.382293701\n",
      "step 300, training accuracy 0.2\n",
      "f : 7290.07714844, q : -37996.1328125, p : -45011.9648438, l : -271.902984619\n",
      "step 400, training accuracy 0.2\n",
      "f : 6957.72070312, q : -36297.0, p : -42686.1367188, l : -266.177825928\n",
      "step 500, training accuracy 0.14\n",
      "f : 6983.53027344, q : -35765.046875, p : -42384.6367188, l : -237.880661011\n",
      "step 600, training accuracy 0.12\n",
      "f : 7013.62011719, q : -35705.34375, p : -42335.9335938, l : -231.459442139\n",
      "step 700, training accuracy 0.2\n",
      "f : 6841.44628906, q : -35543.84375, p : -42197.5273438, l : -253.723052979\n",
      "step 800, training accuracy 0.06\n",
      "f : 6980.39746094, q : -35773.5195312, p : -42838.140625, l : -304.141082764\n",
      "step 900, training accuracy 0.24\n",
      "f : 6982.72314453, q : -35724.7578125, p : -42824.1953125, l : -206.642791748\n",
      "step 1000, training accuracy 0.3\n",
      "f : 6934.86230469, q : -35572.8046875, p : -42357.0273438, l : -200.609405518\n",
      "step 1100, training accuracy 0.28\n",
      "f : 6960.93457031, q : -35741.8046875, p : -42410.0664062, l : -275.297058105\n",
      "step 1200, training accuracy 0.3\n",
      "f : 6898.71240234, q : -35557.25, p : -42761.7890625, l : -193.259796143\n",
      "step 1300, training accuracy 0.24\n",
      "f : 6904.35302734, q : -35666.4882812, p : -42441.5078125, l : -269.842681885\n",
      "step 1400, training accuracy 0.18\n",
      "f : 7005.16259766, q : -35576.7539062, p : -42552.1796875, l : -289.944885254\n",
      "step 1500, training accuracy 0.2\n",
      "f : 6970.20654297, q : -35639.03125, p : -42547.5234375, l : -307.130523682\n",
      "step 1600, training accuracy 0.32\n",
      "f : 7030.63183594, q : -35828.265625, p : -42411.9570312, l : -310.553497314\n",
      "step 1700, training accuracy 0.32\n",
      "f : 7009.41308594, q : -35837.703125, p : -42169.8046875, l : -316.728210449\n",
      "step 1800, training accuracy 0.3\n",
      "f : 7024.921875, q : -35874.7890625, p : -42814.421875, l : -304.683166504\n",
      "step 1900, training accuracy 0.14\n",
      "f : 7032.98730469, q : -35769.484375, p : -42419.4765625, l : -265.058898926\n",
      "step 2000, training accuracy 0.28\n",
      "f : 7009.50341797, q : -35848.4960938, p : -42118.6835938, l : -268.924621582\n",
      "step 2100, training accuracy 0.2\n",
      "f : 7060.41992188, q : -35651.5078125, p : -42265.1953125, l : -257.189605713\n",
      "step 2200, training accuracy 0.14\n",
      "f : 7024.45214844, q : -35762.7890625, p : -42339.9960938, l : -203.782318115\n",
      "step 2300, training accuracy 0.26\n",
      "f : 6986.9453125, q : -35652.84375, p : -42468.5625, l : -270.405059814\n",
      "step 2400, training accuracy 0.26\n",
      "f : 6916.23388672, q : -35509.9296875, p : -42427.8515625, l : -273.014526367\n",
      "step 2500, training accuracy 0.24\n",
      "f : 6890.93603516, q : -36056.4648438, p : -42754.4609375, l : -216.668609619\n",
      "step 2600, training accuracy 0.18\n",
      "f : 6913.63916016, q : -35482.390625, p : -42510.0, l : -315.676696777\n",
      "step 2700, training accuracy 0.2\n",
      "f : 6881.20507812, q : -35491.8984375, p : -42332.0703125, l : -261.769775391\n",
      "step 2800, training accuracy 0.2\n",
      "f : 6989.99707031, q : -35483.9296875, p : -42642.265625, l : -275.720703125\n",
      "step 2900, training accuracy 0.18\n",
      "f : 7154.92578125, q : -35804.1523438, p : -42218.9335938, l : -264.696716309\n",
      "step 3000, training accuracy 0.08\n",
      "f : 6999.58691406, q : -35566.09375, p : -42049.875, l : -260.084289551\n",
      "step 3100, training accuracy 0.28\n",
      "f : 7024.578125, q : -35428.7109375, p : -42675.9101562, l : -210.794830322\n",
      "step 3200, training accuracy 0.18\n",
      "f : 6900.72119141, q : -36089.2734375, p : -42370.4101562, l : -307.248718262\n",
      "step 3300, training accuracy 0.14\n",
      "f : 7015.39355469, q : -35992.9882812, p : -42362.5390625, l : -329.422912598\n",
      "step 3400, training accuracy 0.26\n",
      "f : 7001.55761719, q : -35697.78125, p : -42499.3398438, l : -204.886245728\n",
      "step 3500, training accuracy 0.14\n",
      "f : 6893.89453125, q : -35817.515625, p : -42399.2890625, l : -270.256317139\n",
      "step 3600, training accuracy 0.14\n",
      "f : 6883.6640625, q : -35761.3125, p : -42367.34375, l : -194.193572998\n",
      "step 3700, training accuracy 0.14\n",
      "f : 6964.55175781, q : -35903.0585938, p : -42202.703125, l : -210.278900146\n",
      "step 3800, training accuracy 0.12\n",
      "f : 6834.28466797, q : -35704.9453125, p : -42799.65625, l : -256.969421387\n",
      "step 3900, training accuracy 0.22\n",
      "f : 6952.94628906, q : -35862.4609375, p : -42478.8242188, l : -296.35144043\n",
      "step 4000, training accuracy 0.22\n",
      "f : 6932.87695312, q : -35996.7304688, p : -42757.71875, l : -231.420043945\n",
      "step 4100, training accuracy 0.2\n",
      "f : 6990.16308594, q : -35564.796875, p : -42453.09375, l : -255.72883606\n",
      "step 4200, training accuracy 0.14\n",
      "f : 6973.15673828, q : -36053.15625, p : -42260.5195312, l : -340.646911621\n",
      "step 4300, training accuracy 0.18\n",
      "f : 6995.85839844, q : -35918.6171875, p : -42233.5078125, l : -209.905883789\n",
      "step 4400, training accuracy 0.18\n",
      "f : 6947.92822266, q : -35679.2578125, p : -42577.1289062, l : -250.27053833\n",
      "step 4500, training accuracy 0.08\n",
      "f : 7025.96679688, q : -35720.4765625, p : -42266.9570312, l : -257.29876709\n",
      "step 4600, training accuracy 0.14\n",
      "f : 6994.64208984, q : -35807.2773438, p : -42589.28125, l : -209.715209961\n",
      "step 4700, training accuracy 0.24\n",
      "f : 6881.55029297, q : -35749.28125, p : -42208.9609375, l : -261.939697266\n",
      "step 4800, training accuracy 0.22\n",
      "f : 7116.24902344, q : -35688.53125, p : -42564.5078125, l : -207.927627563\n",
      "step 4900, training accuracy 0.2\n",
      "f : 6866.90722656, q : -35927.9726562, p : -41985.453125, l : -251.211669922\n",
      "step 5000, training accuracy 0.24\n",
      "f : 6923.94238281, q : -36056.2148438, p : -42709.5429688, l : -246.896118164\n",
      "step 5100, training accuracy 0.18\n",
      "f : 6986.67089844, q : -35715.2539062, p : -42231.6171875, l : -287.847961426\n",
      "step 5200, training accuracy 0.24\n",
      "f : 6976.67382812, q : -35813.140625, p : -42677.2929688, l : -304.12399292\n",
      "step 5300, training accuracy 0.18\n",
      "f : 7130.10449219, q : -35844.8984375, p : -42572.8671875, l : -212.203948975\n",
      "step 5400, training accuracy 0.16\n",
      "f : 7061.26855469, q : -35933.6171875, p : -42440.4765625, l : -266.07623291\n",
      "step 5500, training accuracy 0.28\n",
      "f : 6933.25, q : -35677.8671875, p : -42135.375, l : -220.387207031\n",
      "step 5600, training accuracy 0.2\n",
      "f : 6903.86474609, q : -35666.640625, p : -41954.0546875, l : -346.654602051\n",
      "step 5700, training accuracy 0.32\n",
      "f : 6943.04589844, q : -35491.1484375, p : -42541.671875, l : -268.769714355\n",
      "step 5800, training accuracy 0.24\n",
      "f : 6984.95458984, q : -35468.84375, p : -42321.7695312, l : -221.821502686\n",
      "step 5900, training accuracy 0.26\n",
      "f : 6911.68359375, q : -35445.9140625, p : -42258.3632812, l : -314.875244141\n",
      "step 6000, training accuracy 0.26\n",
      "f : 7047.47070312, q : -35747.0585938, p : -42572.578125, l : -283.563293457\n",
      "step 6100, training accuracy 0.3\n",
      "f : 7067.21484375, q : -35957.59375, p : -42580.234375, l : -288.021911621\n",
      "step 6200, training accuracy 0.26\n",
      "f : 6999.49804688, q : -35915.046875, p : -42801.9882812, l : -263.447967529\n",
      "step 6300, training accuracy 0.26\n",
      "f : 6936.13916016, q : -36001.1171875, p : -42164.28125, l : -285.996185303\n",
      "step 6400, training accuracy 0.3\n",
      "f : 6821.18261719, q : -35876.5351562, p : -42507.1328125, l : -278.025909424\n",
      "step 6500, training accuracy 0.2\n",
      "f : 7025.39941406, q : -35931.3867188, p : -42494.7109375, l : -280.952758789\n",
      "step 6600, training accuracy 0.2\n",
      "f : 6979.31054688, q : -35482.8828125, p : -42451.1132812, l : -276.700622559\n",
      "step 6700, training accuracy 0.2\n",
      "f : 7028.67871094, q : -35719.953125, p : -42630.8515625, l : -306.117218018\n",
      "step 6800, training accuracy 0.16\n",
      "f : 6822.12109375, q : -35956.4765625, p : -42559.6835938, l : -244.99609375\n",
      "step 6900, training accuracy 0.12\n",
      "f : 6938.9140625, q : -36021.9609375, p : -42712.1601562, l : -214.28137207\n",
      "step 7000, training accuracy 0.12\n",
      "f : 7171.34667969, q : -35829.3671875, p : -42823.640625, l : -186.131317139\n",
      "step 7100, training accuracy 0.14\n",
      "f : 6861.44287109, q : -35643.3125, p : -42449.8046875, l : -282.263549805\n",
      "step 7200, training accuracy 0.24\n",
      "f : 7008.81835938, q : -35867.34375, p : -42182.7265625, l : -263.250183105\n",
      "step 7300, training accuracy 0.22\n",
      "f : 7064.23535156, q : -35779.78125, p : -42344.4296875, l : -241.570648193\n",
      "step 7400, training accuracy 0.26\n",
      "f : 7084.05566406, q : -35775.8046875, p : -42234.8359375, l : -302.529602051\n",
      "step 7500, training accuracy 0.2\n",
      "f : 6901.38427734, q : -35886.6757812, p : -42242.7265625, l : -227.741577148\n",
      "step 7600, training accuracy 0.16\n",
      "f : 7014.12597656, q : -35616.28125, p : -41939.140625, l : -269.357116699\n",
      "step 7700, training accuracy 0.16\n",
      "f : 6954.78222656, q : -36067.1132812, p : -42281.96875, l : -333.196594238\n",
      "step 7800, training accuracy 0.16\n",
      "f : 7003.40087891, q : -36075.2695312, p : -42705.3515625, l : -228.678344727\n",
      "step 7900, training accuracy 0.18\n",
      "f : 6929.84277344, q : -36040.5234375, p : -42584.1953125, l : -259.101654053\n",
      "step 8000, training accuracy 0.2\n",
      "f : 6957.47705078, q : -35595.1171875, p : -42526.875, l : -293.454406738\n",
      "step 8100, training accuracy 0.22\n",
      "f : 6981.5078125, q : -36126.421875, p : -42634.4179688, l : -299.122924805\n",
      "step 8200, training accuracy 0.12\n",
      "f : 7027.09570312, q : -35547.046875, p : -42413.4609375, l : -227.876617432\n",
      "step 8300, training accuracy 0.28\n",
      "f : 6958.2734375, q : -36147.8203125, p : -42212.6992188, l : -233.633682251\n",
      "step 8400, training accuracy 0.08\n",
      "f : 6973.69042969, q : -35863.8125, p : -42656.1523438, l : -243.295593262\n",
      "step 8500, training accuracy 0.16\n",
      "f : 6981.65722656, q : -35802.0234375, p : -42410.8671875, l : -275.169525146\n",
      "step 8600, training accuracy 0.12\n",
      "f : 7111.7421875, q : -35791.4765625, p : -42445.6015625, l : -302.874633789\n",
      "step 8700, training accuracy 0.2\n",
      "f : 6963.53662109, q : -35747.9921875, p : -42551.0390625, l : -226.026428223\n",
      "step 8800, training accuracy 0.1\n",
      "f : 6889.57421875, q : -35545.9453125, p : -42730.0585938, l : -288.683959961\n",
      "step 8900, training accuracy 0.26\n",
      "f : 6987.80078125, q : -35812.0234375, p : -42541.984375, l : -246.59236145\n",
      "step 9000, training accuracy 0.28\n",
      "f : 6970.77636719, q : -35628.1523438, p : -42086.0664062, l : -317.375\n",
      "step 9100, training accuracy 0.18\n",
      "f : 7034.77734375, q : -35559.9609375, p : -42237.53125, l : -249.721679688\n",
      "step 9200, training accuracy 0.28\n",
      "f : 7078.83007812, q : -35736.9140625, p : -42378.5546875, l : -288.70880127\n",
      "step 9300, training accuracy 0.06\n",
      "f : 7155.32421875, q : -36048.3828125, p : -42723.375, l : -223.340332031\n",
      "step 9400, training accuracy 0.22\n",
      "f : 6961.13183594, q : -35638.890625, p : -42526.46875, l : -230.870773315\n",
      "step 9500, training accuracy 0.24\n",
      "f : 7063.42626953, q : -35900.0234375, p : -42536.609375, l : -277.379943848\n",
      "step 9600, training accuracy 0.16\n",
      "f : 7007.26904297, q : -36053.5429688, p : -42207.4921875, l : -208.693664551\n",
      "step 9700, training accuracy 0.14\n",
      "f : 7026.06835938, q : -35710.6796875, p : -42502.0273438, l : -335.462402344\n",
      "step 9800, training accuracy 0.24\n",
      "f : 6933.25439453, q : -35762.03125, p : -42189.5078125, l : -274.269958496\n",
      "step 9900, training accuracy 0.32\n",
      "f : 7082.45214844, q : -36132.421875, p : -42283.3984375, l : -291.896606445\n",
      "step 10000, training accuracy 0.26\n",
      "f : 7021.24902344, q : -35547.7695312, p : -42610.8671875, l : -274.82019043\n",
      "step 10100, training accuracy 0.2\n",
      "f : 6976.77734375, q : -36125.1523438, p : -42169.8789062, l : -308.106750488\n",
      "step 10200, training accuracy 0.12\n",
      "f : 6989.91796875, q : -35975.75, p : -42418.1953125, l : -246.49105835\n",
      "step 10300, training accuracy 0.2\n",
      "f : 6957.02246094, q : -35805.5078125, p : -42805.3632812, l : -235.464141846\n",
      "step 10400, training accuracy 0.32\n",
      "f : 6958.77539062, q : -35867.5351562, p : -42510.34375, l : -245.621688843\n",
      "step 10500, training accuracy 0.2\n",
      "f : 6977.05859375, q : -35742.8476562, p : -42390.4570312, l : -324.301391602\n",
      "step 10600, training accuracy 0.34\n",
      "f : 7186.68945312, q : -35498.1953125, p : -42475.1328125, l : -266.265136719\n",
      "step 10700, training accuracy 0.34\n",
      "f : 6996.75146484, q : -35690.4453125, p : -42597.5, l : -303.085845947\n",
      "step 10800, training accuracy 0.24\n",
      "f : 6960.48583984, q : -35478.5078125, p : -42709.171875, l : -215.592559814\n",
      "step 10900, training accuracy 0.16\n",
      "f : 7027.45751953, q : -35765.03125, p : -42270.1445312, l : -244.310974121\n",
      "step 11000, training accuracy 0.26\n",
      "f : 7070.32470703, q : -35860.4296875, p : -42798.6953125, l : -293.743347168\n",
      "step 11100, training accuracy 0.12\n",
      "f : 7013.36328125, q : -35876.9257812, p : -42275.265625, l : -257.48449707\n",
      "step 11200, training accuracy 0.3\n",
      "f : 7099.31054688, q : -36164.3515625, p : -42330.6484375, l : -234.584320068\n",
      "step 11300, training accuracy 0.24\n",
      "f : 6847.16308594, q : -35978.4492188, p : -42183.8007812, l : -356.286865234\n",
      "step 11400, training accuracy 0.22\n",
      "f : 6952.23974609, q : -35673.9296875, p : -42429.46875, l : -296.329559326\n",
      "step 11500, training accuracy 0.16\n",
      "f : 6869.59863281, q : -35360.1328125, p : -42716.421875, l : -249.288574219\n",
      "step 11600, training accuracy 0.24\n",
      "f : 6772.28417969, q : -35733.8789062, p : -42492.8984375, l : -263.127685547\n",
      "step 11700, training accuracy 0.14\n",
      "f : 6813.62695312, q : -35952.4492188, p : -42372.9609375, l : -229.593582153\n",
      "step 11800, training accuracy 0.18\n",
      "f : 6929.91113281, q : -35581.6953125, p : -42572.2265625, l : -328.944030762\n",
      "step 11900, training accuracy 0.18\n",
      "f : 7035.15332031, q : -36089.6601562, p : -42471.4492188, l : -269.871948242\n",
      "step 12000, training accuracy 0.14\n",
      "f : 6919.53369141, q : -35943.890625, p : -42608.9375, l : -344.554016113\n",
      "step 12100, training accuracy 0.18\n",
      "f : 6953.22460938, q : -35740.15625, p : -42301.4296875, l : -227.142791748\n",
      "step 12200, training accuracy 0.28\n",
      "f : 7096.02685547, q : -35415.8828125, p : -42174.859375, l : -235.562011719\n",
      "step 12300, training accuracy 0.14\n",
      "f : 7112.01855469, q : -35437.6992188, p : -42676.25, l : -236.220153809\n",
      "step 12400, training accuracy 0.2\n",
      "f : 6881.0625, q : -35819.578125, p : -42519.5664062, l : -280.619873047\n",
      "step 12500, training accuracy 0.24\n",
      "f : 6956.56835938, q : -35736.0234375, p : -42500.9023438, l : -231.709472656\n",
      "step 12600, training accuracy 0.18\n",
      "f : 7003.00292969, q : -35642.8164062, p : -42569.6445312, l : -210.664031982\n",
      "step 12700, training accuracy 0.18\n",
      "f : 6931.18408203, q : -36281.7109375, p : -42519.2109375, l : -192.403076172\n",
      "step 12800, training accuracy 0.3\n",
      "f : 7045.82373047, q : -35867.21875, p : -42733.6210938, l : -267.251586914\n",
      "step 12900, training accuracy 0.22\n",
      "f : 6791.421875, q : -35617.8515625, p : -42234.7617188, l : -293.796142578\n",
      "step 13000, training accuracy 0.1\n",
      "f : 6908.30322266, q : -35597.8320312, p : -42693.8984375, l : -281.771148682\n",
      "step 13100, training accuracy 0.22\n",
      "f : 7111.45605469, q : -35561.9726562, p : -42495.8359375, l : -207.688110352\n",
      "step 13200, training accuracy 0.16\n",
      "f : 7053.61181641, q : -35511.921875, p : -42563.2851562, l : -261.968017578\n",
      "step 13300, training accuracy 0.14\n",
      "f : 7077.05322266, q : -35673.2773438, p : -42534.34375, l : -277.736877441\n",
      "step 13400, training accuracy 0.12\n",
      "f : 6902.02050781, q : -35886.8515625, p : -42435.6289062, l : -269.013824463\n",
      "step 13500, training accuracy 0.18\n",
      "f : 6968.29882812, q : -35261.4648438, p : -42433.6953125, l : -309.088989258\n",
      "step 13600, training accuracy 0.2\n",
      "f : 6901.79980469, q : -36176.5703125, p : -42474.296875, l : -331.861816406\n",
      "step 13700, training accuracy 0.16\n",
      "f : 6994.171875, q : -35624.3671875, p : -42949.53125, l : -300.628173828\n",
      "step 13800, training accuracy 0.3\n",
      "f : 6955.32861328, q : -35671.4453125, p : -42492.9179688, l : -225.002243042\n",
      "step 13900, training accuracy 0.18\n",
      "f : 7030.80273438, q : -35854.2304688, p : -42094.7578125, l : -265.108398438\n",
      "step 14000, training accuracy 0.2\n",
      "f : 7122.47265625, q : -35494.5859375, p : -42297.9414062, l : -224.317214966\n",
      "step 14100, training accuracy 0.16\n",
      "f : 6943.76611328, q : -35959.6796875, p : -42548.21875, l : -239.491729736\n",
      "step 14200, training accuracy 0.24\n",
      "f : 6973.31152344, q : -36091.96875, p : -42572.9179688, l : -292.192169189\n",
      "step 14300, training accuracy 0.22\n",
      "f : 6926.90625, q : -36012.109375, p : -42545.2851562, l : -284.153869629\n",
      "step 14400, training accuracy 0.16\n",
      "f : 7087.234375, q : -35614.640625, p : -42464.9375, l : -325.651489258\n",
      "step 14500, training accuracy 0.28\n",
      "f : 7043.52880859, q : -35871.0, p : -42174.8828125, l : -290.560577393\n",
      "step 14600, training accuracy 0.3\n",
      "f : 6953.02050781, q : -35702.234375, p : -42734.9414062, l : -293.686340332\n",
      "step 14700, training accuracy 0.18\n",
      "f : 6999.11621094, q : -35877.1289062, p : -42525.7265625, l : -228.005645752\n",
      "step 14800, training accuracy 0.22\n",
      "f : 6863.36035156, q : -36070.2421875, p : -42239.0273438, l : -331.850036621\n",
      "step 14900, training accuracy 0.18\n",
      "f : 6983.10888672, q : -35333.5195312, p : -42050.6835938, l : -192.550628662\n",
      "step 15000, training accuracy 0.2\n",
      "f : 6961.71972656, q : -35511.3242188, p : -43101.1171875, l : -268.489227295\n",
      "step 15100, training accuracy 0.14\n",
      "f : 7092.93261719, q : -35440.28125, p : -42548.5703125, l : -207.724456787\n",
      "step 15200, training accuracy 0.24\n",
      "f : 6958.0, q : -35965.265625, p : -42610.2109375, l : -290.097930908\n",
      "step 15300, training accuracy 0.22\n",
      "f : 6924.60644531, q : -35726.3046875, p : -42489.78125, l : -230.503936768\n",
      "step 15400, training accuracy 0.24\n",
      "f : 6975.01953125, q : -35972.3867188, p : -42263.1328125, l : -226.537689209\n",
      "step 15500, training accuracy 0.18\n",
      "f : 7030.39941406, q : -35409.7617188, p : -42753.6679688, l : -262.886260986\n",
      "step 15600, training accuracy 0.26\n",
      "f : 7145.1796875, q : -35565.328125, p : -42796.9101562, l : -191.861343384\n",
      "step 15700, training accuracy 0.26\n",
      "f : 6914.66113281, q : -35689.421875, p : -42409.5820312, l : -306.28503418\n",
      "step 15800, training accuracy 0.32\n",
      "f : 6961.40332031, q : -35877.171875, p : -42442.109375, l : -207.211212158\n",
      "step 15900, training accuracy 0.16\n",
      "f : 7048.26855469, q : -36071.359375, p : -42488.0390625, l : -258.305908203\n",
      "step 16000, training accuracy 0.12\n",
      "f : 7011.83496094, q : -35790.40625, p : -42501.84375, l : -254.367324829\n",
      "step 16100, training accuracy 0.06\n",
      "f : 6990.21679688, q : -35281.359375, p : -42460.9296875, l : -233.938049316\n",
      "step 16200, training accuracy 0.2\n",
      "f : 7068.3046875, q : -35376.015625, p : -42346.0117188, l : -232.941238403\n",
      "step 16300, training accuracy 0.2\n",
      "f : 7064.42626953, q : -35826.1328125, p : -42322.3789062, l : -199.443725586\n",
      "step 16400, training accuracy 0.24\n",
      "f : 6942.36230469, q : -35646.8164062, p : -42345.5703125, l : -315.50302124\n",
      "step 16500, training accuracy 0.16\n",
      "f : 6938.31054688, q : -35821.671875, p : -42788.578125, l : -256.694793701\n",
      "step 16600, training accuracy 0.4\n",
      "f : 7018.23632812, q : -35916.0625, p : -42619.0546875, l : -290.342803955\n",
      "step 16700, training accuracy 0.2\n",
      "f : 7008.08496094, q : -35866.9101562, p : -42765.8125, l : -198.011749268\n",
      "step 16800, training accuracy 0.18\n",
      "f : 6988.375, q : -35623.40625, p : -42211.578125, l : -206.106903076\n",
      "step 16900, training accuracy 0.18\n",
      "f : 7210.4453125, q : -36077.8632812, p : -42675.3984375, l : -236.32244873\n",
      "step 17000, training accuracy 0.2\n",
      "f : 7049.84570312, q : -35451.203125, p : -42558.34375, l : -342.387695312\n",
      "step 17100, training accuracy 0.12\n",
      "f : 6976.79052734, q : -35632.5390625, p : -42631.6875, l : -340.25177002\n",
      "step 17200, training accuracy 0.34\n",
      "f : 6929.22851562, q : -35872.671875, p : -42325.5390625, l : -211.30607605\n",
      "step 17300, training accuracy 0.22\n",
      "f : 6928.92431641, q : -35617.6796875, p : -42343.0546875, l : -288.324707031\n",
      "step 17400, training accuracy 0.16\n",
      "f : 7048.00097656, q : -35780.9765625, p : -42387.65625, l : -221.514968872\n",
      "step 17500, training accuracy 0.16\n",
      "f : 7033.31396484, q : -36230.1953125, p : -42229.7109375, l : -184.40411377\n",
      "step 17600, training accuracy 0.2\n",
      "f : 6914.58349609, q : -35776.8359375, p : -42565.3515625, l : -251.05809021\n",
      "step 17700, training accuracy 0.34\n",
      "f : 6966.45605469, q : -35630.8359375, p : -42667.078125, l : -224.974441528\n",
      "step 17800, training accuracy 0.12\n",
      "f : 7050.16308594, q : -35698.4179688, p : -42676.765625, l : -219.469238281\n",
      "step 17900, training accuracy 0.14\n",
      "f : 6924.91601562, q : -35718.6289062, p : -42084.171875, l : -240.865402222\n",
      "step 18000, training accuracy 0.24\n",
      "f : 6985.31103516, q : -35765.75, p : -42451.1875, l : -320.649261475\n",
      "step 18100, training accuracy 0.22\n",
      "f : 6952.43164062, q : -35802.2421875, p : -42356.1289062, l : -299.216461182\n",
      "step 18200, training accuracy 0.14\n",
      "f : 7003.0390625, q : -35518.0117188, p : -42485.6757812, l : -251.014450073\n",
      "step 18300, training accuracy 0.14\n",
      "f : 7003.80957031, q : -35600.5742188, p : -42683.0351562, l : -230.929641724\n",
      "step 18400, training accuracy 0.18\n",
      "f : 7054.50195312, q : -35616.0273438, p : -42321.078125, l : -265.447753906\n",
      "step 18500, training accuracy 0.24\n",
      "f : 7002.78808594, q : -35801.0, p : -42240.8125, l : -234.72819519\n",
      "step 18600, training accuracy 0.16\n",
      "f : 6981.61767578, q : -35780.5546875, p : -42508.6953125, l : -214.41078186\n",
      "step 18700, training accuracy 0.24\n",
      "f : 7072.18261719, q : -35282.59375, p : -42679.0703125, l : -273.807922363\n",
      "step 18800, training accuracy 0.26\n",
      "f : 6879.1015625, q : -35984.515625, p : -42307.0820312, l : -222.615844727\n",
      "step 18900, training accuracy 0.2\n",
      "f : 6935.61181641, q : -35744.8710938, p : -42914.5078125, l : -312.062133789\n",
      "step 19000, training accuracy 0.1\n",
      "f : 6893.68066406, q : -35247.1328125, p : -42492.84375, l : -228.06060791\n",
      "step 19100, training accuracy 0.18\n",
      "f : 6956.06103516, q : -35585.8125, p : -42378.875, l : -263.768676758\n",
      "step 19200, training accuracy 0.2\n",
      "f : 6823.04443359, q : -35589.359375, p : -42522.1601562, l : -219.086181641\n",
      "step 19300, training accuracy 0.18\n",
      "f : 7021.47705078, q : -35526.3203125, p : -42426.2382812, l : -199.607513428\n",
      "step 19400, training accuracy 0.22\n",
      "f : 6909.30419922, q : -35843.2421875, p : -42822.6484375, l : -232.623016357\n",
      "step 19500, training accuracy 0.2\n",
      "f : 6961.42480469, q : -36049.6679688, p : -42699.0507812, l : -250.211395264\n",
      "step 19600, training accuracy 0.26\n",
      "f : 7020.66210938, q : -36055.1757812, p : -42599.6054688, l : -241.296325684\n",
      "step 19700, training accuracy 0.26\n",
      "f : 7020.27587891, q : -35900.6015625, p : -42498.296875, l : -202.069580078\n",
      "step 19800, training accuracy 0.06\n",
      "f : 7013.69726562, q : -35534.1835938, p : -42501.6054688, l : -285.764312744\n",
      "step 19900, training accuracy 0.18\n",
      "f : 6957.43066406, q : -35688.046875, p : -42710.8554688, l : -307.86605835\n",
      "test accuracy 0.1724\n"
     ]
    }
   ],
   "source": [
    "fs = list()\n",
    "qs = list()\n",
    "ps = list()\n",
    "ls = list()\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    feed = {bnn.x: batch[0], bnn.t: batch[1]}\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        train_accuracy = bnn.validate(feed)\n",
    "        v_f, v_q, v_p, v_l = bnn.get_fqpl(feed)\n",
    "        fs.append(v_f), qs.append(v_q), ps.append(v_p), ls.append(v_l)\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"f : {}, q : {}, p : {}, l : {}\".format(v_f, v_q, v_p, v_l))\n",
    "        \n",
    "    \n",
    "    bnn.train(feed)\n",
    "\n",
    "print(\"test accuracy %g\"%bnn.validate({bnn.x: mnist.test.images, bnn.t: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEACAYAAABhzAtFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3325c118d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fs, 'r')\n",
    "plt.plot(qs, 'b')\n",
    "plt.plot(ps, 'g')\n",
    "plt.plot(ls, 'k')\n",
    "plt.legend(['f', 'q', 'p', 'l'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.range(300)\n",
    "aa = tf.reshape(a, [30, 10])\n",
    "aaa = tf.tile(tf.expand_dims(aa, 0), [10, 1, 1])\n",
    "\n",
    "b = tf.truncated_normal([30, 10], stddev = 0.2)\n",
    "\n",
    "argmaxbs = tf.argmax(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(30), Dimension(10)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2, ..., 297, 298, 299], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(aaa, [-1]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298   5  11  27  32  43  57\n",
      "  63  76  84  97 105 118 123 138 142 158 164 172 182 193 201 215 227 237\n",
      " 248 258 261 271 281 298   5  11  27  32  43  57  63  76  84  97 105 118\n",
      " 123 138 142 158 164 172 182 193 201 215 227 237 248 258 261 271 281 298\n",
      "   5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298   5  11  27  32  43  57\n",
      "  63  76  84  97 105 118 123 138 142 158 164 172 182 193 201 215 227 237\n",
      " 248 258 261 271 281 298   5  11  27  32  43  57  63  76  84  97 105 118\n",
      " 123 138 142 158 164 172 182 193 201 215 227 237 248 258 261 271 281 298\n",
      "   5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298   5  11  27  32  43  57\n",
      "  63  76  84  97 105 118 123 138 142 158 164 172 182 193 201 215 227 237\n",
      " 248 258 261 271 281 298   5  11  27  32  43  57  63  76  84  97 105 118\n",
      " 123 138 142 158 164 172 182 193 201 215 227 237 248 258 261 271 281 298\n",
      "   5  11  27  32  43  57  63  76  84  97 105 118 123 138 142 158 164 172\n",
      " 182 193 201 215 227 237 248 258 261 271 281 298]\n",
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 5, 3, 8, 6, 1, 0, 7, 0, 3, 7, 8, 0, 4, 5, 8, 8, 0, 2, 6, 7, 7,\n",
       "       0, 3, 7, 8, 3, 8, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = tf.range(30).eval()\n",
    "br = ar * 10 + argmaxbs.eval()\n",
    "cr = tf.tile(br, [10])\n",
    "\n",
    "result = tf.gather(tf.reshape(aaa, [-1]), cr).eval()\n",
    "print result\n",
    "print result.shape\n",
    "argmaxbs.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 35], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(aa, [[0, 1], [3, 5]]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test =  mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(tf.truncated_normal([2, 3], stddev = 0.1))\n",
    "b = tf.Variable(tf.constant(-1.0, shape = [2, 3]))\n",
    "c = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (1,) and () are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-50d057430d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36marg_max\u001b[1;34m(input, dimension, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m   \"\"\"\n\u001b[0;32m    163\u001b[0m   result = _op_def_lib.apply_op(\"ArgMax\", input=input, dimension=dimension,\n\u001b[1;32m--> 164\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m    165\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    701\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    702\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m                            op_def=op_def)\n\u001b[0m\u001b[0;32m    704\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2317\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2319\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2320\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1709\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1710\u001b[0m                          % op.type)\n\u001b[1;32m-> 1711\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1712\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36m_ArgOpShape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1858\u001b[0m   \u001b[1;34m\"\"\"Common shape function for arg-reduction ops.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1859\u001b[0m   \u001b[0mdimension_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m   \u001b[0mdimension_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1861\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \"\"\"\n\u001b[0;32m    740\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (1,) and () are incompatible"
     ]
    }
   ],
   "source": [
    "tf.argmax(c, [1]).eval(feed_dict = {c: [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [15, 14, 13, 12, 11, 10, 10, 10, 10, 10]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(a).eval()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519]]\n",
      "[[-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]\n",
      " [-0.04836457  0.11699509 -0.10365508 -0.04836457  0.11699509 -0.10365508\n",
      "  -0.04836457  0.11699509 -0.10365508]\n",
      " [-0.11315618 -0.14960578 -0.13785519 -0.11315618 -0.14960578 -0.13785519\n",
      "  -0.11315618 -0.14960578 -0.13785519]]\n",
      "[[[-0.04836457  0.11699509 -0.10365508]\n",
      "  [-0.11315618 -0.14960578 -0.13785519]]\n",
      "\n",
      " [[-0.04836457  0.11699509 -0.10365508]\n",
      "  [-0.11315618 -0.14960578 -0.13785519]]]\n",
      "[[ 0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519]]\n",
      "[[ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]\n",
      " [ 0.04836457 -0.11699509  0.10365508  0.04836457 -0.11699509  0.10365508\n",
      "   0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519  0.11315618  0.14960578  0.13785519\n",
      "   0.11315618  0.14960578  0.13785519]]\n",
      "[[ 0.04836457 -0.11699509  0.10365508]\n",
      " [ 0.11315618  0.14960578  0.13785519]]\n",
      "[[ 0.04836457  0.11315618]\n",
      " [-0.11699509  0.14960578]\n",
      " [ 0.10365508  0.13785519]]\n"
     ]
    }
   ],
   "source": [
    "with sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    print a.eval()\n",
    "    print tf.tile(a, [6, 3]).eval()\n",
    "    print tf.tile(tf.expand_dims(a, 0), [2, 1, 1]).eval()\n",
    "    a = a * b\n",
    "    print a.eval()\n",
    "    print tf.tile(a, [6, 3]).eval()\n",
    "    print tf.transpose(a, [0, 1]).eval()\n",
    "    print tf.transpose(a, [1, 0]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 2, 3, 4, 5]\n",
    "print arr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 15\n",
    "\n",
    "\n",
    "fc8_n_param   = [4096, n_features]\n",
    "\n",
    "images = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "labels = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "fc7 = net.layers['fc7']\n",
    "\n",
    "\n",
    "W_fc8 = weight_variable(fc8_n_param)\n",
    "b_fc8 = bias_variable([fc8_n_param[-1]])\n",
    "\n",
    "fc8_pred = tf.matmul(fc7, W_fc8) + b_fc8\n",
    "\n",
    "mse_loss = tf.reduce_mean(tf.square(labels - fc8_pred))\n",
    "learning_rate = 1e-4\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(mse_loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
